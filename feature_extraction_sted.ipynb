{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "from skimage.exposure import rescale_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load images from H5 + normalize intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(arr, min_, max_):\n",
    "    '''\n",
    "    non-clipping linear normalization, min_ will be set to 0, max_ to 1 in the output \n",
    "    '''\n",
    "    return (arr - min_) / (max_ - min_)\n",
    "\n",
    "def load_all_details(in_file, do_normalize=True, percentiles=(2.5, 99.5)):\n",
    "    \n",
    "    imgs = []\n",
    "    percentiles_ = []\n",
    "    names = []\n",
    "    \n",
    "    with h5.File(in_file, 'r') as fd:\n",
    "        details = [k for k in fd['experiment'].keys() if 'detail' in k]\n",
    "\n",
    "        for detail_name in details:\n",
    "            data = fd['experiment/{}/0/0'.format(detail_name)][...].astype(np.float32)\n",
    "            \n",
    "            percentiles_img = np.percentile(data, percentiles)\n",
    "            \n",
    "            # rescale intensity            \n",
    "            if do_normalize:\n",
    "                data = normalize(data, *percentiles_img)\n",
    "            \n",
    "            imgs.append(data.squeeze())\n",
    "            percentiles_.append(percentiles_img) # save percentiles in raw intensity (even if we )\n",
    "            names.append(detail_name) # save dataset name\n",
    "            \n",
    "    return imgs, percentiles_, names\n",
    "\n",
    "def get_quantiles_per_replicate(file_to_imgs_map, percentiles=(2.5, 99.5)):\n",
    "    replicate_to_imgs = defaultdict(list)\n",
    "    for f, imgs in file_to_imgs_map.items():\n",
    "        replicate_to_imgs[Path(f).parent].extend(imgs)\n",
    "    \n",
    "    res_quantiles = {}\n",
    "    for rep, imgs in replicate_to_imgs.items():\n",
    "        flat_vals = np.concatenate([img.flat for img in imgs])\n",
    "        res_quantiles[rep] = np.percentile(flat_vals, percentiles)\n",
    "    \n",
    "    return res_quantiles\n",
    "\n",
    "\n",
    "def normalize_loaded_per_replicate(loaded, percentiles):\n",
    "    quantiles_per_rep = get_quantiles_per_replicate({k: imgs for k, (imgs, _, _) in loaded.items()}, percentiles)\n",
    "    \n",
    "    res = {}\n",
    "    for i, (f, (imgs, p_raw, names)) in enumerate(loaded.items()):\n",
    "        perc = tuple(quantiles_per_rep[Path(f).parent])\n",
    "        imgs_rescaled = [normalize(img, *perc) for img in imgs]\n",
    "        res[f] = imgs_rescaled, p_raw, names\n",
    "        \n",
    "        print(f'({i+1}/{len(loaded)}): {f}')\n",
    "\n",
    "    return res, quantiles_per_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get sorted list of hdf5 files\n",
    "NOTE: the folder structure should be ```.../biological_replicate_id(date_condition)/technical_replicate_id/random_file_hash.h5```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "in_files = glob.glob('/scratch/hoerl/auto_sir_dna_comp/*/*/*.h5')\n",
    "in_files.sort()\n",
    "in_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check that all files have the same directory depth\n",
    "np.unique([len(Path(in_file).parents) for in_file in in_files], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentiles for normalization\n",
    "percentiles=(2.5, 99.8)\n",
    "# whether to normalize per replicate (True) or per image (False)\n",
    "normalize_per_replicate = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# single-threaded version:\n",
    "# loaded = {f: load_all_details(f) for f in in_files}\n",
    "\n",
    "loaded = {}\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    futures = [tpe.submit(load_all_details, f, not normalize_per_replicate, percentiles) for f in in_files]\n",
    "    for i, (f,future) in enumerate(zip(in_files, futures)):\n",
    "        loaded[f] = future.result()    \n",
    "        print(f'({i+1}/{len(futures)}): {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize per replicate\n",
    "If we have not normalized per image, normalize per replicate now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_per_replicate:\n",
    "    loaded, quantiles_per_rep = normalize_loaded_per_replicate(loaded, percentiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot examples of images per file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_img_grid(imgs, **kwargs):\n",
    "    \n",
    "    imgs = imgs.copy()\n",
    "    random.shuffle(imgs)\n",
    "    \n",
    "    fig, axs = plt.subplots(**kwargs)\n",
    "\n",
    "    for ax, img in zip(axs.flat, imgs):\n",
    "        ax.imshow(np.clip(img.squeeze(), 0, 1), cmap='gray')\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(loaded.items()):\n",
    "    print(k)\n",
    "    plot_img_grid(v[0], ncols=4, nrows=1, figsize=(12,4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Simple segmentation via threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu, threshold_li, gaussian\n",
    "from skimage.transform import rescale\n",
    "from skimage.morphology import remove_small_holes, remove_small_objects\n",
    "import tqdm\n",
    "\n",
    "def blur_and_segment(img, blur_sigma=16, max_hole_size=512, min_object_size=512):\n",
    "\n",
    "    img = img.squeeze()\n",
    "\n",
    "    g_ = gaussian(img, blur_sigma)\n",
    "    \n",
    "    # clip and convert to 8-bit\n",
    "    # otherwise, li thresholding kept running veeery long for a few images\n",
    "    g_ = (np.clip(g_, 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    mask = g_ > threshold_li(g_)\n",
    "\n",
    "    # a bit of binary cleaning\n",
    "    mask = remove_small_objects(mask, min_object_size)\n",
    "    mask = remove_small_holes(mask, max_hole_size)\n",
    "\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment multithreaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seg_sigma = 12\n",
    "max_hole_size = 5000\n",
    "min_object_size = 5000\n",
    "\n",
    "segs = {}\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    for i, (k, v) in enumerate(sorted(loaded.items())):\n",
    "        futures = [tpe.submit(blur_and_segment, vi, seg_sigma, max_hole_size, min_object_size) for vi in v[0]]\n",
    "        segs[k] = [f.result() for f in futures]\n",
    "        print(f'({i+1}/{len(loaded)}): {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize segmentation on example images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import gray2rgb, label2rgb\n",
    "\n",
    "def plot_seg_grid(imgs, **kwargs):\n",
    "    \n",
    "    imgs = imgs.copy()\n",
    "    random.shuffle(imgs)\n",
    "    \n",
    "    fig, axs = plt.subplots(**kwargs)\n",
    "    for ax, (img, seg) in zip(axs.flat, imgs):\n",
    "        \n",
    "#         img = np.clip(img, 0, 1)\n",
    "        # rescale to 0,1 for better visibility\n",
    "        img = rescale_intensity(img, out_range=(0,1))\n",
    "        \n",
    "        # use skimage label draw tools\n",
    "        lab_img = label2rgb(seg*1, gray2rgb(img), bg_label=0)\n",
    "        ax.imshow(lab_img)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for k, v in sorted(segs.items()):\n",
    "    print(k)\n",
    "    plot_seg_grid(v, ncols=4, nrows=1, figsize=(12,4))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Extract texture features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def get_glcm_features(img, mask, props, distances, angles, blur_sigma=None):\n",
    "    \n",
    "    # do a bit of blur anyway to reduce color quantization effects on GLCM\n",
    "    if blur_sigma is None:\n",
    "        blur_sigma = 0.5\n",
    "\n",
    "    # clip and make uint8 here\n",
    "    img = (np.clip(gaussian(img, blur_sigma), 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    # make input for masked GLCM:\n",
    "    # 1) set bg to zero\n",
    "    # 2) set everything else +1 \n",
    "    # (NB: should not be necessary if mask comes from threshold, but let's keep it anyway)\n",
    "    img_for_masked_glcm = img.copy().astype(np.uint16)\n",
    "    img_for_masked_glcm[~ mask] = 0\n",
    "    img_for_masked_glcm[mask] += 1 \n",
    "\n",
    "    # get glcm, but ignore first row & column (co-ocurrence with 0 := background)\n",
    "    glcm = greycomatrix(img_for_masked_glcm, distances, angles, 257)\n",
    "    glcm = glcm[1:,1:]\n",
    "    \n",
    "    return np.stack([greycoprops(glcm, prop=p) for p in props])\n",
    "\n",
    "def get_lbp_histogram(img, mask, blur_sigma=None, Rs=3, P=32):\n",
    "\n",
    "    # ensure Rs is iterable even if we only have a single R\n",
    "    if not isinstance(Rs, Iterable):\n",
    "        Rs = [Rs]\n",
    "    \n",
    "    # do a bit of blur anyway to reduce color quantization effects on GLCM\n",
    "    if blur_sigma is None:\n",
    "        blur_sigma = 0.5\n",
    "        \n",
    "    # clip and make uint8 here\n",
    "    img = (np.clip(gaussian(img, blur_sigma), 0, 1) * 255).astype(np.uint8)\n",
    "    \n",
    "    probs = []\n",
    "    for R in Rs:\n",
    "        lbp = local_binary_pattern(img, P, R, method='uniform')\n",
    "        probs_i, bins = np.histogram(lbp[mask], bins=np.arange(P+3), density=True)\n",
    "        probs.append(probs_i)\n",
    "\n",
    "    return np.stack(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to None to not blur images, code for sigma estimation below\n",
    "blur_sigma = None\n",
    "# blur_sigma = 5.2\n",
    "\n",
    "# GLCM feature options\n",
    "props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "\n",
    "# distances = [2, 4, 7, 12, 16]\n",
    "distances = [2, 4, 8, 16, 32, 64]\n",
    "angles = [0, np.pi/2]\n",
    "\n",
    "# LBP feature options\n",
    "add_lbps = False\n",
    "Rs = [2,4,6]\n",
    "P = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate GLCM features for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "glcms = {}\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    for i,(k, v) in enumerate(sorted(segs.items())):\n",
    "        futures = [tpe.submit(get_glcm_features, i, m, props, distances, angles, blur_sigma) for (i,m) in v]\n",
    "        glcms[k] = [f.result() for f in futures]\n",
    "        print(f'({i+1}/{len(loaded)}): {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: calculate LBP features\n",
    "\n",
    "Did not imporove results much, can be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only calculate if we want them\n",
    "if add_lbps:\n",
    "    lbps = {}\n",
    "    with ThreadPoolExecutor() as tpe:\n",
    "        for k, v in sorted(segs.items()):\n",
    "            futures = [tpe.submit(get_lbp_histogram, i, m, blur_sigma, Rs, P) for (i,m) in v]\n",
    "            lbps[k] = [f.result() for f in futures]\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Some simple other features + save as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import maximum_filter1d\n",
    "\n",
    "def get_simple_features(img, mask, blur_sigma=None):\n",
    "    \n",
    "    # do a bit of blur anyway to reduce color quantization effects on GLCM\n",
    "    if blur_sigma is None:\n",
    "        blur_sigma = 0.5\n",
    "\n",
    "    img = gaussian(img, blur_sigma)\n",
    "        \n",
    "    return np.mean(img[mask]), np.std(img[mask]), np.sum(mask), img.shape[0], img.shape[1]\n",
    "\n",
    "def get_num_blank_lines(img, normalization_vals=(0,1), thresh=1.0, max_filter_size=5):\n",
    "    \n",
    "    # un-normalize image that has been inensity-scaled to (0,1) based on original values\n",
    "    # assumes no clipping has happened\n",
    "    # use the default values (0,1) to use the image as-is\n",
    "    low_val, high_val = normalization_vals\n",
    "    norm_range = high_val - low_val\n",
    "    img_unnormalized = img * norm_range + low_val\n",
    "    \n",
    "    # get mean along rows and cols\n",
    "    # max-filter to minimize effect of single blank lines\n",
    "    # (the artifacts we observe are usually many consecutive blank lines due to detector shutdown or scanning outside the FOV)\n",
    "    row_means = img_unnormalized.mean(axis=1)\n",
    "    col_means = img_unnormalized.mean(axis=0)\n",
    "    row_means = maximum_filter1d(row_means, max_filter_size)\n",
    "    col_means = maximum_filter1d(col_means, max_filter_size)\n",
    "    \n",
    "    # result: numer of rows, cols with mean intensity under thresh (blank)\n",
    "    return (row_means < thresh).sum(), (col_means < thresh).sum() \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the multiple dicts + calculate simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# column names for GLCM feats\n",
    "props_names = list(map(lambda x: '_'.join(x), product(props, map(str, distances), map(str, map(np.rad2deg, angles)))))\n",
    "\n",
    "df_dict = defaultdict(list)\n",
    "\n",
    "for i, (k,v) in enumerate(glcms.items()):\n",
    "    for vi in v:\n",
    "        df_dict['filename'].append(k)\n",
    "        for value, prop_name in zip(np.array(vi).flat, props_names):\n",
    "            df_dict[prop_name].append(value)\n",
    "    \n",
    "    if add_lbps:\n",
    "        for lbp in lbps[k]:\n",
    "            for value, (R, p) in zip(lbp.flat, product(Rs, np.arange(P+2, dtype=int))):\n",
    "                df_dict[f'LBP_R{R}_{p}'] = value\n",
    "    \n",
    "    for (img, mask) in segs[k]:\n",
    "        mu, sig, area, height, width = get_simple_features(img, mask, blur_sigma)\n",
    "        df_dict['intensity_mu'].append(mu)\n",
    "        df_dict['intensity_sigma'].append(sig)\n",
    "        df_dict['mask_area'].append(area)\n",
    "        df_dict['img_height'].append(height)\n",
    "        df_dict['img_width'].append(width)\n",
    "    \n",
    "    imgs, percs, names = loaded[k]\n",
    "    \n",
    "    if normalize_per_replicate:\n",
    "        percs_rep_low, percs_rep_high = quantiles_per_rep[Path(k).parent]\n",
    "    \n",
    "    for (per_low, per_high), name, img in zip(percs, names, imgs):\n",
    "        df_dict['dataset_name'].append(name)\n",
    "        df_dict['perc_low'].append(per_low if not normalize_per_replicate else percs_rep_low)\n",
    "        df_dict['perc_high'].append(per_high if not normalize_per_replicate else percs_rep_high)\n",
    "        df_dict['perc_low_image'].append(per_low)\n",
    "        df_dict['perc_high_image'].append(per_high)\n",
    "        \n",
    "        blank_rows, blank_cols = get_num_blank_lines(img, (per_low, per_high) if not normalize_per_replicate else (percs_rep_low, percs_rep_high))\n",
    "        df_dict['num_blank_rows'].append(blank_rows)\n",
    "        df_dict['num_blank_cols'].append(blank_cols)\n",
    "        \n",
    "    print(f'({i+1}/{len(glcms)}): {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONALLY: load existing table and append (e.g. when adding new replicates)\n",
    "\n",
    "append_to_existing_df = False\n",
    "existing_df = '/scratch/hoerl/auto_sir_dna_comp/20211111_glcm_all_lithreshold_smallblur.csv'\n",
    "\n",
    "if append_to_existing_df:\n",
    "    df = pd.read_csv(existing_df).append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('/scratch/hoerl/auto_sir_dna_comp/20220829_glcm-long_all_replicatenorm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: save pngs for manual sorting, confocal blur simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a random sample of images for manual sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat, chain, zip_longest\n",
    "from skimage.io import imsave\n",
    "import os\n",
    "\n",
    "sorting_out_dir = '/scratch/hoerl/auto_sir_dna_comp/sorting20210316'\n",
    "\n",
    "# flat list of (image, dataset_name, hdf5_filename)\n",
    "imgsplusnames = list(chain(*[zip(v[0], v[2], repeat(k)) for k,v in sorted(loaded.items())]))\n",
    "random.shuffle(imgsplusnames)\n",
    "\n",
    "if not os.path.exists(sorting_out_dir):\n",
    "    os.makedirs(sorting_out_dir)\n",
    "\n",
    "for img, name, file in imgsplusnames[:250]:\n",
    "    outname = (os.path.split(file)[1].replace('.h5', '') + '_' + name + '.png')\n",
    "    img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "    # NOTE: uncomment this line to actually save\n",
    "#     imsave(os.path.join(sorting_out_dir, outname), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check sorting\n",
    "\n",
    "In the following cells, we load a feature table with an ```classification_auto```-column (as produced in ```subset_selection2.ipynb```) select a random sample from the images we loaded above and save them as .png for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_cls = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_all_imagenorm_withcls.csv')\n",
    "df_with_cls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_sample = 250\n",
    "\n",
    "dfs_good = df_with_cls[df_with_cls.classification_auto == 'good'].sample(n_to_sample)\n",
    "dfs_bad = df_with_cls[df_with_cls.classification_auto == 'bad'].sample(n_to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat, chain, zip_longest\n",
    "from skimage.io import imsave\n",
    "import os\n",
    "\n",
    "imgsplusnames = list(chain(*[zip(v[0], v[2], repeat(k)) for k,v in sorted(loaded.items())]))\n",
    "\n",
    "good_out_dir = '/scratch/hoerl/auto_sir_dna_comp/sorting20211115_resorted_val/good'\n",
    "if not os.path.exists(good_out_dir):\n",
    "    os.makedirs(good_out_dir)\n",
    "\n",
    "for ridx, r in dfs_good.iterrows():\n",
    "    img = next((i for (i, n, f) in imgsplusnames if n==r['dataset_name'] and f==r['filename']))\n",
    "    outname = (os.path.split(r['filename'])[1].replace('.h5', '') + '_' + r['dataset_name'] + '.png')\n",
    "    img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "    imsave(os.path.join(good_out_dir, outname), img)\n",
    "    \n",
    "bad_out_dir = '/scratch/hoerl/auto_sir_dna_comp/sorting20211115_resorted_val/bad'\n",
    "if not os.path.exists(bad_out_dir):\n",
    "    os.makedirs(bad_out_dir)\n",
    "\n",
    "for ridx, r in dfs_bad.iterrows():\n",
    "    img = next((i for (i, n, f) in imgsplusnames if n==r['dataset_name'] and f==r['filename']))\n",
    "    outname = (os.path.split(r['filename'])[1].replace('.h5', '') + '_' + r['dataset_name'] + '.png')\n",
    "    img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "    imsave(os.path.join(bad_out_dir, outname), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get pixel sizes, calculate blur sigma for simulated confocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print pixel sizes for first STED image in all input files\n",
    "\n",
    "for file in in_files:\n",
    "    with h5.File(file, 'r') as fd:\n",
    "        details = [k for k in fd['experiment'].keys() if 'detail' in k]\n",
    "        \n",
    "        if len(details) == 0:\n",
    "            continue\n",
    "        \n",
    "        first_detail = details[0]\n",
    "\n",
    "        dataset = fd['experiment/{}/0'.format(first_detail)]\n",
    "        meta = json.loads(dataset.attrs['measurement_meta'])\n",
    "        print(file, float(meta['ExpControl']['scan']['range']['x']['psz']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumed FWHMS STED 50nm, conf 250nm\n",
    "sted_fwhm = 5e-8\n",
    "conf_fwhm = 2.5e-7\n",
    "pixel_size = 2e-8\n",
    "\n",
    "def fwhm_to_sigma(f):\n",
    "    return f / (2 * np.sqrt(2 * np.log(2)))\n",
    "    \n",
    "# correct for already existing STED FWHM\n",
    "blur_sigma = np.sqrt(fwhm_to_sigma(conf_fwhm)**2 - fwhm_to_sigma(sted_fwhm)**2)\n",
    "\n",
    "# additional blur to get conf resolution\n",
    "blur_sigma = blur_sigma / pixel_size\n",
    "blur_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot an example of STED vs. simulated confocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_plot = next(iter(loaded.values()))[0]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(16,8))\n",
    "axs[0].imshow(img_plot[0].squeeze(), cmap='gray')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('STED raw')\n",
    "axs[1].imshow(gaussian(img_plot[0].squeeze(), 5.2), cmap='gray')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('simulated confocal')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
