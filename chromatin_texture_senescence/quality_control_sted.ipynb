{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "from scipy.optimize import leastsq\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load unsorted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_all_replicatenorm.csv')\n",
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_all_replicatenorm_confocalblur.csv')\n",
    "df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_all_imagenorm.csv')\n",
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_all_imagenorm_confocalblur.csv')\n",
    "\n",
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220829_glcm-long_all_replicatenorm.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## table cleaning\n",
    "\n",
    "We first unify some dataset/replicate naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to fix some incorrectly formatted dates \n",
    "correct_len = lambda s: s if len(s) == 8 else s.replace('2020', '20200')\n",
    "\n",
    "# parse preparation date and replicate id from filename \n",
    "dates = df.filename.apply(lambda s: correct_len(s.split('/')[-3].split('_')[0]))\n",
    "replicate = df.filename.apply(lambda s: 'rep' + re.split('rep', s.split('/')[-2])[-1].replace('_', '')[:1])\n",
    "\n",
    "# combine (biological replicate) preparation date + replicate id -> technical replicate identifier\n",
    "replicate = dates + '_' + replicate\n",
    "\n",
    "# hacky solution: add 'w' suffix to 6-well samples\n",
    "replicate += np.where(df.filename.str.split('rep', expand=True)[1].str[1] == 'w', 'w', '')\n",
    "\n",
    "# get cell_class (treatment/condition) from filename\n",
    "cell_class = df.filename.apply(lambda s: '_'.join(s.split('/')[-3].split('_')[1:]))\n",
    "\n",
    "# unify some condition names\n",
    "cell_class = cell_class.str.replace('day', 'd_ICM_young')\n",
    "cell_class[cell_class == 'IMR90_young_untr'] = 'IMR90_young_untreated'\n",
    "cell_class[cell_class == 'IMR90_old'] = 'IMR90_untreated_old'\n",
    "cell_class[cell_class == 'IMR90_6d_ICM'] = 'IMR90_6d_ICM_young'\n",
    "cell_class[cell_class == 'IMR90_young'] = 'IMR90_young_untreated'\n",
    "\n",
    "# condition:= biological replicate id\n",
    "df['condition'] = dates + '_' + cell_class\n",
    "df['replicate'] = replicate\n",
    "df['cell_class'] = cell_class\n",
    "\n",
    "# add foreground mean_value\n",
    "# newer versions of GLCM normalize intenisty to 0-1 instead of old versions that did 0-255\n",
    "# can be detected through presence of 'perc_{low/high}_image' column\n",
    "rescale_factor = 1.0 if 'perc_low_image' in df.columns else 255.0\n",
    "\n",
    "df['fg_mean'] = df.intensity_mu / rescale_factor * (df.perc_high - df.perc_low) + df.perc_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cell_class.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load manual sorting from JSON\n",
    "\n",
    "soring JSON files should contain lists of manually classified examples and follow the format: \n",
    "```\n",
    "{\n",
    "   'good' : [\n",
    "       [filename, hdf5_dataset_name],\n",
    "       ...\n",
    "   ],\n",
    "   'bad': [\n",
    "       ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/scratch/hoerl/auto_sir_dna_comp/sorting20211115_resorted.json', 'r') as fd:\n",
    "    sorting_dict = json.load(fd)\n",
    "\n",
    "def get_classification_from_dict(row, sorting_dict):\n",
    "    filename = os.path.split(row.filename)[1].replace('.h5', '')\n",
    "    dataset_name = row.dataset_name\n",
    "\n",
    "    if [filename, dataset_name] in sorting_dict['good']:\n",
    "        return 'good'\n",
    "    elif [filename, dataset_name] in sorting_dict['bad']:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'unclassified'\n",
    "\n",
    "df['classification_manual'] = df.apply(lambda row: get_classification_from_dict(row, sorting_dict), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple count of items per class\n",
    "for k, v in sorting_dict.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for classification from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get only feature column values\n",
    "Xs = df.drop(columns=['filename', 'dataset_name', 'condition', 'replicate', 'cell_class', 'classification_manual']).values\n",
    "\n",
    "# we have some NaNs, impute them\n",
    "Xs = Imputer().fit_transform(Xs)\n",
    "# scale\n",
    "Xs = StandardScaler().fit_transform(Xs)\n",
    "\n",
    "# only get Xs for which we have a manual classification\n",
    "has_goodbad = df.classification_manual.apply(lambda r: r in ['good', 'bad']).values\n",
    "Xs_goodbad = Xs[has_goodbad]\n",
    "\n",
    "# encode good/bad as 1/0\n",
    "# NOTE: we manually fit to ensure good=1\n",
    "le_goodbad = LabelEncoder().fit(['bad', 'good'])\n",
    "ys_goodbad = le_goodbad.transform(df.classification_manual.values[has_goodbad])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init classifier and get total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "\n",
    "# get classifier performance via cross validation\n",
    "np.mean(cross_val_score(model, Xs_goodbad, ys_goodbad))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR-curve and optimal threshold selection\n",
    "\n",
    "We select probability threshold so we achieve a target precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = cross_val_predict(model, Xs_goodbad, ys_goodbad, method='predict_proba')\n",
    "prec, rec, thresh = precision_recall_curve(ys_goodbad, probs[:,1])\n",
    "\n",
    "# get lowest threshold with precision over target of 0.95\n",
    "prec_target = 0.95\n",
    "idx = np.argmax(prec > prec_target)\n",
    "\n",
    "plt.plot(rec, prec)\n",
    "plt.annotate(f'precision: {prec[idx]}\\nrecall: {rec[idx]}\\n@thresh: {thresh[idx]}', [rec[idx], prec[idx]],\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05), xytext=(0.2, 0.85));\n",
    "plt.xlabel('Recall');\n",
    "plt.ylabel('Precision');\n",
    "plt.title('PR-curve: good/bad classification');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infer quality on the rest of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# default: pick probability threshold to reach target precision\n",
    "prob_tresh = thresh[idx]\n",
    "# prob_tresh = 0.5\n",
    "\n",
    "# fit good/bad model on whole train set\n",
    "model_goodbad = model\n",
    "model_goodbad.fit(Xs_goodbad, ys_goodbad)\n",
    "\n",
    "# get prediction for all other rows\n",
    "ys_pred = model_goodbad.predict_proba(Xs)[:,1] > prob_tresh\n",
    "df['classification_auto'] = le_goodbad.inverse_transform(ys_pred * 1)\n",
    "\n",
    "# subset of good datapoints\n",
    "df_good = df[df['classification_auto'] == 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "sorted(zip(model_goodbad.feature_importances_, df.drop(['filename', 'dataset_name', 'condition', 'replicate', 'cell_class'], axis=1).columns), reverse=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe\n",
    "Output of this is used in ```ananlysis_v2.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the good examples to CSV\n",
    "df_good.to_csv('/scratch/hoerl/auto_sir_dna_comp/20220829_glcm-long_good95_replicatenorm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole df with classification, e.g. to look at examples\n",
    "# df.to_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_all_imagenorm_withcls.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting statistics\n",
    "optionally only for images older than a given date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_datfiltered = df\n",
    "# df_datfiltered = df[df.replicate.apply(lambda s: parser.parse(s.split('_')[0])) < parser.parse('20201201')]\n",
    "# df_datfiltered = df[df.replicate.apply(lambda s: parser.parse(s.split('_')[0])) > parser.parse('20210801')]\n",
    "\n",
    "groupby_rows = ['cell_class', 'condition']\n",
    "# groupby_rows = ['cell_class']\n",
    "\n",
    "badsum = df_datfiltered[df_datfiltered['classification_auto'] == 'bad'].groupby(groupby_rows).classification_auto.describe()\n",
    "goodsum = df_datfiltered[df_datfiltered['classification_auto'] == 'good'].groupby(groupby_rows).classification_auto.describe()\n",
    "\n",
    "summary = pd.DataFrame({'bad': badsum.freq, 'good': goodsum.freq})\n",
    "summary = summary.fillna(value=0)\n",
    "summary['total'] = summary.good + summary.bad\n",
    "summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intenisty histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# gerneral plot parameters\n",
    "figsize = (15, 10)\n",
    "xlim_for_plot = (0, 750)\n",
    "bins_for_plot = 50\n",
    "\n",
    "plot_vline = False\n",
    "vline_location = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Histograms per cell_class\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "axs = df_good.hist('fg_mean', by=['cell_class',], ax=plt.gca(), sharex=True, bins=bins_for_plot, density=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlim(xlim_for_plot)\n",
    "    if plot_vline:\n",
    "        ax.axvline(vline_location, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) histograms grouped by replicate\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, sharex=True, figsize=figsize)\n",
    "for (i, dfi), ax in zip(df_good.groupby('cell_class'), axs.flat):\n",
    "    for (rep, dfj) in dfi.groupby('replicate'):\n",
    "        ax.hist(dfj.fg_mean.values, density=True, alpha=0.5,\n",
    "                bins=np.linspace(*xlim_for_plot, bins_for_plot), label=rep)\n",
    "    ax.set_title(i)\n",
    "    ax.set_xlim(xlim_for_plot)\n",
    "    if plot_vline:\n",
    "        ax.axvline(vline_location, color='red')\n",
    "    ax.legend()\n",
    "\n",
    "# hide unnecessary last subplot\n",
    "axs[-1,-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) plot histogram outlines\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, sharex=True, figsize=figsize)\n",
    "for (i, dfi), ax in zip(df_good.groupby('cell_class'), axs.flat):\n",
    "    for (rep, dfj) in dfi.groupby('replicate'):\n",
    "        h, bins = np.histogram(dfj.fg_mean.values, density=True, bins=np.linspace(*xlim_for_plot, bins_for_plot+1))\n",
    "        ax.plot((bins[1:] + bins[:-1])/2, h,label=rep)\n",
    "    ax.set_title(i)\n",
    "    ax.set_xlim(xlim_for_plot)\n",
    "    if plot_vline:\n",
    "        ax.axvline(vline_location, color='red')\n",
    "    ax.legend()\n",
    "    \n",
    "axs[-1,-1].set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Subset of data based on intensity\n",
    "\n",
    "We also tried subsequent steps only on subsets of the data with similar intensities.\n",
    "\n",
    "The following cells can be used to create a subset of the data with either mean foreground intensity close to a predefined target value or find an intensity value for which we have the smallest deviation across the whole dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Pick subsample closest to target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n_per_class = 150\n",
    "target_value = 50\n",
    "\n",
    "df_good['diff'] = np.abs(df_good.fg_mean - target_value)\n",
    "\n",
    "# for each (biological) replicate, pick the datapoints with the smallest difference to target_value\n",
    "dfis = []\n",
    "for i, dfi in df_good.groupby('condition'):\n",
    "    dfis.append(dfi.sort_values('diff').iloc[:n_per_class])\n",
    "\n",
    "df_selected = pd.concat(dfis).drop(['diff'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.to_csv('C:/Users/hoerl/Downloads/20210419_glcm_all_extrafeats_selected50intensity.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) pick subsample and target value with minimal total differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_good.cell_class.unique()\n",
    "n_best = 250\n",
    "\n",
    "def get_sum_diff(target_intensity, df_good):\n",
    "    dfs = {}\n",
    "    df_good = df_good.copy()\n",
    "    df_good['diff'] = np.abs(df_good.fg_mean - target_intensity)\n",
    "#     for idx, dfi in df_good.groupby(['cell_class', 'replicate']):\n",
    "    for idx, dfi in df_good.groupby(['cell_class']): \n",
    "        df_best = dfi.sort_values('diff').reset_index(drop=True).loc[:(n_best-1), :]\n",
    "        dfs[idx] = df_best\n",
    "\n",
    "    sum_dev = reduce(add, [np.sum(v['diff']) for k, v in dfs.items()])\n",
    "    return sum_dev, dfs\n",
    "\n",
    "# single parameter version\n",
    "f = lambda ti : get_sum_diff(ti, df_good)[0]\n",
    "\n",
    "# optimize target_intensity\n",
    "optimal_ti, _ = leastsq(f, 50)\n",
    "_, dfs = get_sum_diff(optimal_ti, df_good)\n",
    "optimal_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = reduce(pd.DataFrame.append, [df for _, df in dfs.items()]).drop(['diff'], 1)\n",
    "df_selected.to_csv('/Users/david/Downloads/20210707_glcm_good_selected_intensity_percondition.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
