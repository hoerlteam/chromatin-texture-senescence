{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate subset of DNA texture files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy msrs\n",
    "\n",
    "r = '/data/cooperation_data/Preliminary_projects/AgyrisPapantonis_ChromatinTexture_AgeingCells/auto_sir_ageing-cells/*'\n",
    "root_dir = '/data/cooperation_data/Preliminary_projects/AgyrisPapantonis_ChromatinTexture_AgeingCells/auto_sir_ageing-cells/*'\n",
    "out = '/scratch/hoerl/dna_sir_ageing/examples'\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "for ri in glob.glob(r):\n",
    "    \n",
    "    outi = out + '/' + ri.rsplit('/')[-1]\n",
    "    \n",
    "    rii = glob.glob(ri + '/*/*detail*')\n",
    "    rii = random.sample(rii, 10)\n",
    "    \n",
    "#     print(rii)\n",
    "#     os.makedirs(outi, exist_ok=True)\n",
    "#     for riii in rii:\n",
    "#         shutil.copy2(riii, outi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scaled_gray_colormap(_min, _max):\n",
    "    '''\n",
    "    FIXME: colormap as expected by tifffile imsave?\n",
    "    Does not work in macOS Preview though...\n",
    "    '''\n",
    "    cm = np.full((3, 2**16), 2**8-1, dtype=np.uint16)\n",
    "    cm[:,:_min] = 0\n",
    "    cm[:, _min:_max] = np.linspace(0, 2**8-1, _max - _min)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "\n",
    "# load manual sorting\n",
    "try:\n",
    "    with open('/scratch/hoerl/dna_sir_ageing/examples_tiff/20201006sorting.json', 'r') as fd:\n",
    "        sorting = json.load(fd)\n",
    "except FileNotFoundError:\n",
    "    warnings.warn('No manual sorting found, classification will not work')\n",
    "    sorting = {'good': [], 'bad': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from skimage.filters import threshold_otsu\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io.tifffile import imsave, TiffWriter, TiffFile\n",
    "from skimage.transform import resize\n",
    "from skimage.exposure import rescale_intensity\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "\n",
    "good_imgs = []\n",
    "bad_imgs = []\n",
    "\n",
    "size = (32,32)\n",
    "\n",
    "def safe_otsu(img):\n",
    "    try:\n",
    "        return threshold_otsu(img)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def get_features(img, size):\n",
    "    img_rescaled = resize(img, size, order=1, mode='reflect', anti_aliasing=True, clip=False, preserve_range=True)\n",
    "    \n",
    "    img_size = img.size\n",
    "    black_size = np.sum(img==0)\n",
    "    \n",
    "    fg_area = np.sum(img_rescaled >= safe_otsu(img_rescaled))\n",
    "    fg_mean = np.mean(img_rescaled[img_rescaled >= safe_otsu(img_rescaled)])\n",
    "    bg_mean = np.mean(img_rescaled[img_rescaled < safe_otsu(img_rescaled)])\n",
    "    fg_mean = 0 if np.isnan(fg_mean) else fg_mean\n",
    "    bg_mean = 0 if np.isnan(bg_mean) else bg_mean\n",
    "    \n",
    "    return img_size, black_size, fg_area, fg_mean, bg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = defaultdict(list)\n",
    "for class_dir in glob.glob(root_dir):\n",
    "    \n",
    "    # get h5 files one or two levels down\n",
    "    h5s = glob.glob(class_dir + '/*/*.h5') + glob.glob(class_dir + '/*/*/*.h5')\n",
    "\n",
    "    for h5i in h5s:\n",
    "        with h5.File(h5i, 'r') as fd:\n",
    "            \n",
    "            # get all detail images\n",
    "            details = [l for l in list(fd['experiment'].keys()) if 'detail' in l]\n",
    "            \n",
    "            with ThreadPoolExecutor() as tpe:\n",
    "                futures = []\n",
    "                for d in details:\n",
    "                    dat = fd['experiment'][d]['0']['0']\n",
    "                    img = dat[...].squeeze()\n",
    "                    futures.append(tpe.submit(get_features, img, size))\n",
    "                \n",
    "                for d,f in zip(details, futures):\n",
    "                    img_size, black_size, fg_area, fg_mean, bg_mean = f.result()\n",
    "                    \n",
    "                    if '_'.join([pathlib2.Path(h5i).name[:-3], d]) in sorting['good']:\n",
    "                        classification = 'good'\n",
    "                        good_imgs.append(fd['experiment'][d]['0']['0'][...].squeeze())\n",
    "                        \n",
    "                    elif '_'.join([pathlib2.Path(h5i).name[:-3], d]) in sorting['bad']:\n",
    "                        classification = 'bad'\n",
    "                        bad_imgs.append(fd['experiment'][d]['0']['0'][...].squeeze())\n",
    "                    else:\n",
    "                        classification = 'unknown'\n",
    "                        \n",
    "                    res['h5path'].append(h5i)\n",
    "                    res['name'].append(d)\n",
    "                    res['classification'].append(classification)\n",
    "                    res['img_size'].append(img_size)\n",
    "                    res['black_size'].append(black_size)\n",
    "                    res['fg_area'].append(fg_area)\n",
    "                    res['fg_mean'].append(fg_mean)\n",
    "                    res['bg_mean'].append(bg_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.DataFrame.from_dict(res)\n",
    "\n",
    "# get manually annotated subset\n",
    "df_for_classification = df[df.classification.apply(lambda c: c in ['good', 'bad'])]\n",
    "# xs = df_for_classification[['bg_mean', 'fg_mean', 'fg_area', 'black_size', 'img_size']].values\n",
    "xs = df_for_classification[['black_size', 'img_size', 'fg_area']].values\n",
    "ys = (df_for_classification.classification == 'good').values\n",
    "\n",
    "# fit tree\n",
    "cls = DecisionTreeClassifier()\n",
    "cls.fit(xs, ys)\n",
    "\n",
    "# predict for all\n",
    "# xs_all = df[['bg_mean', 'fg_mean', 'fg_area', 'black_size', 'img_size']].values\n",
    "xs_all = df[['black_size', 'img_size', 'fg_area']].values\n",
    "class_predicted = ['good' if yp else 'bad' for yp in cls.predict(xs_all)]\n",
    "df['classification_predicted'] = class_predicted\n",
    "\n",
    "cell_class = df.h5path.apply(lambda p: p.split(os.sep)[-3] if not 'test' in p else p.split(os.sep)[-4])\n",
    "replicate = df.h5path.apply(lambda p: p.split(os.sep)[-2].split('_')[-1] if not 'test' in p else p.split(os.sep)[-3].split('_')[-1])\n",
    "\n",
    "\n",
    "df['cell_class'] = cell_class\n",
    "df['replicate'] = replicate\n",
    "df['fg_mean_sub_bg'] = df.fg_mean - df.bg_mean\n",
    "\n",
    "df_good = df[df.classification_predicted == 'good']\n",
    "# to skip classificiation\n",
    "df_good = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cell_class')['classification_predicted'].describe()\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/scratch/hoerl/dna_sir_ageing/20210430summary_withnew_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/scratch/hoerl/dna_sir_ageing/20210430summary_withnew_samples.csv')\n",
    "df_good = df[df.classification_predicted == 'good']\n",
    "df_good.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good.groupby(['cell_class', 'replicate']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlim_for_plot = (0, 750)\n",
    "bins_for_plot = 50\n",
    "vline_location = 50\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "axs = df_good.hist('fg_mean_sub_bg', by=['cell_class',], ax=plt.gca(), sharex=True, bins=30, density=True)\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlim(xlim_for_plot)\n",
    "    ax.axvline(vline_location, color='red')\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(4, 2, sharex=True, figsize=(10,10))\n",
    "for (i, dfi), ax in zip(df_good.groupby('cell_class'), axs.flat):\n",
    "    for (rep, dfj) in dfi.groupby('replicate'):\n",
    "        ax.hist(dfj.fg_mean_sub_bg.values, density=True, alpha=0.5,\n",
    "                bins=np.linspace(*xlim_for_plot, bins_for_plot), label=rep)\n",
    "    ax.set_title(i)\n",
    "    ax.set_xlim(xlim_for_plot)\n",
    "axs[0,0].legend()\n",
    "axs[-1,-1].set_visible(False)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, sharex=True, figsize=(10,10))\n",
    "for (i, dfi), ax in zip(df_good.groupby('cell_class'), axs.flat):\n",
    "    for (rep, dfj) in dfi.groupby('replicate'):\n",
    "        ax.hist(dfj.fg_mean_sub_bg.values, density=True, alpha=0.5,\n",
    "                bins=np.linspace(*xlim_for_plot, bins_for_plot), label=rep)\n",
    "        # hist outlines - does not look as nice imho\n",
    "#         h, bins = np.histogram(dfj.fg_mean_sub_bg.values, density=True, bins=np.linspace(0,250))\n",
    "#         ax.plot((bins[1:] + bins[:-1])/2, h,label=rep)\n",
    "    ax.set_title(i)\n",
    "    ax.set_xlim(xlim_for_plot)\n",
    "    ax.axvline(vline_location, color='red')\n",
    "axs[0,0].legend()\n",
    "axs[-1,-1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good.groupby(['cell_class',]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_intensity = 50\n",
    "n_best = 250\n",
    "\n",
    "#classes = ['2020622_IMR90_untreated_old', '2020705_IMR90_young_untreated']\n",
    "# classes = ['2020622_IMR90_untreated_old', '2020705_IMR90_young_untreated', '2020625_IMR90_3d_ICM_young', '2020629_IMR90_6d_ICM_young', '2020702_IMR90_9d_ICM_young']\n",
    "# classes = ['20201208_IMR90_3day', '20201214_IMR90_9day', '2020622_IMR90_untreated_old']\n",
    "classes = ['20210326_IMR90_young_untr', '20210402_IMR90_old']\n",
    "df_good['diff'] = np.abs(df_good.fg_mean_sub_bg - target_intensity)\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for ci, dfi in df_good.groupby('cell_class'):\n",
    "    if ci in classes:\n",
    "        df_best = dfi.sort_values('diff').reset_index(drop=True).loc[:(n_best-1), :]\n",
    "        dfs[ci] = df_best\n",
    "        \n",
    "#dfs\n",
    "[len(v) for v in dfs.values()], [(ci, len(dfi)) for ci, dfi in df_good.groupby('cell_class') if ci in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from functools import reduce\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "\n",
    "classes = df_good.cell_class.unique()\n",
    "n_best = 75\n",
    "\n",
    "def get_sum_diff(target_intensity, df_good):\n",
    "    dfs = {}\n",
    "    df_good = df_good.copy()\n",
    "    df_good['diff'] = np.abs(df_good.fg_mean_sub_bg - target_intensity)\n",
    "    for (ci, ri), dfi in df_good.groupby(['cell_class', 'replicate']):\n",
    "#         if ci in classes:\n",
    "            df_best = dfi.sort_values('diff').reset_index(drop=True).loc[:(n_best-1), :]\n",
    "            dfs[(ci, ri)] = df_best\n",
    "\n",
    "    sum_dev = reduce(add, [np.sum(v['diff']) for k, v in dfs.items()])\n",
    "    return sum_dev, dfs\n",
    "\n",
    "f = lambda ti : get_sum_diff(ti, df_good)[0]\n",
    "\n",
    "optimal_ti, _ = leastsq(f, 50)\n",
    "_, dfs = get_sum_diff(optimal_ti, df_good)\n",
    "optimal_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dfs.items():\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "for cell_type, dfi in dfs.items():\n",
    "    \n",
    "    out = '/scratch/hoerl/dna_sir_ageing/examples_tiff_n_200_optimal_intensity_16bit/{}'.format(cell_type)\n",
    "    os.makedirs(out, exist_ok=True)\n",
    "    \n",
    "    for h5file, name, rep in zip(dfi.h5path, dfi.name, dfi.replicate):\n",
    "        #print(rep)\n",
    "        \n",
    "        with h5.File(h5file, 'r') as fd:\n",
    "            dat = fd['experiment'][name]['0']['0']\n",
    "            img = dat[...].astype(np.uint16)\n",
    "#             img_uint8 = rescale_intensity(img, out_range='uint8')\n",
    "#             img_uint8 = img_uint8.astype(np.uint8)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                imsave(os.path.join(out, h5file.rsplit(os.sep)[-1][:-3] + '_' + rep + '_' + name + '.tif'), img, compress=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "for (cell_type, rep), dfi in dfs.items():\n",
    "    \n",
    "    out = '/scratch/hoerl/dna_sir_ageing/examples_tiff_n_200_optimal_intensity_8bit_repl/{}'.format(cell_type + '_' + rep)\n",
    "    os.makedirs(out, exist_ok=True)\n",
    "    \n",
    "    for h5file, name, rep in zip(dfi.h5path, dfi.name, dfi.replicate):\n",
    "        #print(rep)\n",
    "        \n",
    "        with h5.File(h5file, 'r') as fd:\n",
    "            dat = fd['experiment'][name]['0']['0']\n",
    "            img = dat[...].astype(np.uint16)\n",
    "            img_uint8 = rescale_intensity(img, in_range=tuple(np.quantile(img, [0.01, 0.995])), out_range='uint8')\n",
    "            img_uint8 = img_uint8.astype(np.uint8)\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter('ignore')\n",
    "                imsave(os.path.join(out, h5file.rsplit(os.sep)[-1][:-3] + '_' + rep + '_' + name + '.tif'), img_uint8, compress=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC: plot a few good/bad images for each cell class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_to_plot = 5\n",
    "\n",
    "for ci, dfi in df.groupby('cell_class'):\n",
    "    good_i = dfi[dfi.classification_predicted == 'good']\n",
    "    bad_i = dfi[dfi.classification_predicted == 'bad']\n",
    "    print(ci, len(good_i), len(bad_i))\n",
    "    \n",
    "    good_s = good_i.sample(samples_to_plot)\n",
    "    bad_s = bad_i.sample(samples_to_plot)\n",
    "    \n",
    "    for h5file, name, rep in zip(good_s.h5path, good_s.name, good_s.replicate):\n",
    "        with h5.File(h5file, 'r') as fd:\n",
    "            dat = fd['experiment'][name]['0']['0']\n",
    "            img = dat[...]\n",
    "            plt.figure()\n",
    "            plt.imshow(img.squeeze(), cmap='gray')\n",
    "            plt.show()\n",
    "            \n",
    "    for h5file, name, rep in zip(bad_s.h5path, bad_s.name, bad_s.replicate):\n",
    "        with h5.File(h5file, 'r') as fd:\n",
    "            dat = fd['experiment'][name]['0']['0']\n",
    "            img = dat[...]\n",
    "            plt.figure()\n",
    "            plt.imshow(img.squeeze(), cmap='magma')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_size = 8\n",
    "size = (32,32)\n",
    "for g in random.sample(good_imgs, sample_size):\n",
    "    plt.figure()\n",
    "    g = resize(g, size, order=1, mode='reflect', anti_aliasing=True, clip=False, preserve_range=True)\n",
    "    #print(np.mean(g[g > threshold_otsu(g)]), np.mean(g[g < threshold_otsu(g)]))\n",
    "    plt.imshow(g > threshold_otsu(g), cmap='Greens')\n",
    "print()\n",
    "for b in random.sample(bad_imgs, sample_size):\n",
    "    plt.figure()\n",
    "    b = resize(b, size, order=1, mode='reflect', anti_aliasing=True, clip=False, preserve_range=True)\n",
    "    #print(np.mean(b[b > threshold_otsu(b)]), np.mean(b[b < threshold_otsu(b)]))\n",
    "    plt.imshow(b > threshold_otsu(b), cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (32,32)\n",
    "\n",
    "good_feats = np.array([resize(g, size, order=1, mode='reflect', anti_aliasing=True, clip=False, preserve_range=True) for g in good_imgs]).reshape((len(good_imgs), -1))\n",
    "bad_feats = np.array([resize(b, size, order=1, mode='reflect', anti_aliasing=True, clip=False, preserve_range=True) for b in bad_imgs]).reshape((len(bad_imgs), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xs = np.concatenate([good_feats, bad_feats])\n",
    "ys = np.concatenate([np.full((len(good_imgs),), 1), np.full((len(bad_imgs),), 0)])\n",
    "\n",
    "img_sizes = np.concatenate([np.array([[i.size] for i in good_imgs]), np.array([[i.size] for i in bad_imgs])])\n",
    "black_sizes = np.concatenate([np.array([[np.sum(i==0)] for i in good_imgs]), np.array([[np.sum(i==0)] for i in bad_imgs])])\n",
    "\n",
    "fg_area = np.array([[np.sum(i >= safe_otsu(i))] for i in xs])\n",
    "fg_mean = np.array([[np.mean(i[i >= safe_otsu(i)])] for i in xs])\n",
    "bg_mean = np.array([[np.mean(i[i < safe_otsu(i)])] for i in xs])\n",
    "\n",
    "fg_mean[np.isnan(fg_mean)] = 0\n",
    "bg_mean[np.isnan(bg_mean)] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(xs)\n",
    "\n",
    "xs = pca.transform(xs)[:,:16]\n",
    "\n",
    "xs = np.concatenate([ fg_mean, bg_mean, fg_area, img_sizes, black_sizes], 1)\n",
    "\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cross_val_score(cls, xs, ys)\n",
    "\n",
    "#(fg_mean - bg_mean).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/data/cooperation_data/Preliminary_projects/AgyrisPapantonis_ChromatinTexture_AgeingCells/auto_sir_ageing-cells/*'\n",
    "out = '/scratch/hoerl/dna_sir_ageing/examples_tiff'\n",
    "\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.external.tifffile import imsave, TiffWriter, TiffFile\n",
    "from skimage.transform import resize\n",
    "from skimage.exposure import rescale_intensity\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "n_images_per_class = 2\n",
    "#cm = make_scaled_gray_colormap(0, 300)\n",
    "\n",
    "for ri in glob.glob(root_dir):\n",
    "    \n",
    "    outi = out + '/' + ri.rsplit('/')[-1]\n",
    "    h5s = glob.glob(ri + '/*/*.h5') + glob.glob(ri + '/*/*/*.h5')\n",
    "    \n",
    "    # get all (h5-file, image-name) pairs\n",
    "    selection = []\n",
    "    for h5i in h5s:\n",
    "        with h5.File(h5i, 'r') as fd:\n",
    "            details = [l for l in list(fd['experiment'].keys()) if 'detail' in l]\n",
    "            selection.extend(zip(repeat(h5i), details))\n",
    "            \n",
    "    selection = random.sample(selection, n_images_per_class)\n",
    "    os.makedirs(outi, exist_ok=True)\n",
    "    \n",
    "    for h5i, name in selection:\n",
    "        with h5.File(h5i, 'r') as fd:\n",
    "            dat = fd['experiment'][name]['0']['0']\n",
    "            img = dat[...].astype(np.uint16)\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(resize(img.squeeze(), (32,32), order=1, clip=False))\n",
    "            \n",
    "            # rescaled from image range to uint8, otherwise e.g. Texture features in CellProfiler often\n",
    "            # return 1/0?\n",
    "            img_uint8 = rescale_intensity(img, out_range='uint8')\n",
    "            img_uint8 = img_uint8.astype(np.uint8)\n",
    "            #imsave(os.path.join(outi, h5i.rsplit(os.sep)[-1][:-3] + '_' + name + '.tif'), img)\n",
    "            #imsave(os.path.join(outi, h5i.rsplit(os.sep)[-1][:-3] + '_' + name + '8bit.tif'), img_uint8)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
