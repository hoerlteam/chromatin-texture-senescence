{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.filters import threshold_otsu, gaussian\n",
    "from skimage.transform import rescale\n",
    "from skimage.morphology import remove_small_holes, remove_small_objects\n",
    "from skimage.feature import greycomatrix, greycoprops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Get features from already resaved TIFF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/david/Downloads/examples_tiff_300new_8bit/20201208_IMR90_3day/'\n",
    "files = glob.glob(os.path.join(root, '*.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_and_segment(image_path):\n",
    "\n",
    "    img = imread(image_path).squeeze()\n",
    "    blur_sigma = 8 # amount of blur before thresholding\n",
    "\n",
    "    # TODO: maybe downsample for speed?\n",
    "    # img = rescale(img, 0.125, clip=False, preserve_range=True)\n",
    "\n",
    "\n",
    "\n",
    "    g_ = gaussian(img, blur_sigma)\n",
    "    mask = g_ > threshold_otsu(g_)\n",
    "\n",
    "    # a bit of binary cleaning - TODO: check sizes?\n",
    "    mask = remove_small_objects(mask, 512)\n",
    "    mask = remove_small_holes(mask, 512)\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "img, mask = load_img_and_segment(files[25])\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "axs[0].imshow(img)\n",
    "axs[1].imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glcm_features(img, mask):\n",
    "    \n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "    distances = [2, 3, 5, 7, 9]\n",
    "    angles = [0, np.pi/2]\n",
    "    \n",
    "    # make input for masked GLCM:\n",
    "    # 1) set bg to zero\n",
    "    # 2) set everything else +1 \n",
    "    # (NB: should not be necessary if mask comes from threshold, but let's keep it anyway)\n",
    "    img_for_masked_glcm = img.copy().astype(np.uint16)\n",
    "    img_for_masked_glcm[~ mask] = 0\n",
    "    img_for_masked_glcm[mask] += 1 \n",
    "\n",
    "    # get glcm, but ignore first row & column (co-ocurrence with 0 := background)\n",
    "    glcm = greycomatrix(img_for_masked_glcm, distances, angles, 257)\n",
    "    glcm = glcm[1:,1:]\n",
    "    \n",
    "    return np.stack([greycoprops(glcm, prop=p) for p in props])\n",
    "\n",
    "get_glcm_features(img, mask).shape\n",
    "# plt.imshow(img_for_masked_glcm)\n",
    "# np.sum(glcm), np.prod(img.shape)\n",
    "# np.sum(glcm), np.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = '/Users/david/Downloads/examples_tiff_300new_8bit/20201208_IMR90_3day/'\n",
    "files = glob.glob(os.path.join('/Users/david/Downloads/examples_tiff_300new_8bit/20201208_IMR90_3day/', '*.tif'))\n",
    "files += glob.glob(os.path.join('/Users/david/Downloads/examples_tiff_300new_8bit/20201214_IMR90_9day/', '*.tif'))\n",
    "# files += glob.glob(os.path.join('/Users/david/Downloads/examples_tiff_300new_8bit/2020622_IMR90_untreated_old/', '*.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def analysis(img_path):\n",
    "    img, mask = load_img_and_segment(img_path)\n",
    "    # use all-ones mask to disable masking\n",
    "#     mask = np.ones(img.shape, dtype=bool)\n",
    "    return get_glcm_features(img, mask)\n",
    "\n",
    "res = []\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    futures = [tpe.submit(analysis, f) for f in files]\n",
    "    for (f,p) in tqdm.tqdm(zip(futures, files), total=len(files)):\n",
    "        res.append((p, f.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.array([f for _,f in res]).reshape((len(files), -1))\n",
    "conditions = [f.split(os.sep)[-2] for f in files]\n",
    "replicates = [f.split(os.sep)[-1].split('_')[1] for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALTERNATIVE: load saved GLCM values from analysis directly from h5\n",
    "\n",
    "Tables generated by ```glcm_from_h5.ipynb```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "\n",
    "df = pd.read_csv('C:/Users/hoerl/Downloads/20210419_glcm_all_extrafeats_selected50intensity.csv')\n",
    "df['condition'] = [f.split('/')[-3] for f in df.filename]\n",
    "\n",
    "# only select a subset of conditions\n",
    "# TODO: do selection after good/bad cls?\n",
    "\n",
    "# selected_conditions = ['2020705_IMR90_young_untreated', '2020629_IMR90_6d_ICM_young',\n",
    "#                     '2020625_IMR90_3d_ICM_young', '2020622_IMR90_untreated_old',\n",
    "#                     '20201214_IMR90_9day', '2020702_IMR90_9d_ICM_young', '20201208_IMR90_3day',\n",
    "#                       '20210326_IMR90_young_untr', '20210402_IMR90_old']\n",
    "\n",
    "# selected_conditions = ['2020705_IMR90_young_untreated', '2020629_IMR90_6d_ICM_young',\n",
    "#                     '2020625_IMR90_3d_ICM_young', '2020622_IMR90_untreated_old',\n",
    "#                      '2020702_IMR90_9d_ICM_young', '20210326_IMR90_young_untr', '20210402_IMR90_old']\n",
    "\n",
    "# selected_conditions = ['2020705_IMR90_young_untreated', '2020622_IMR90_untreated_old',]\n",
    "\n",
    "selected_conditions = ['20210326_IMR90_young_untr', '20210402_IMR90_old','2020705_IMR90_young_untreated', '2020622_IMR90_untreated_old']\n",
    "\n",
    "\n",
    "\n",
    "# selected_conditions = ['2020622_IMR90_untreated_old', '20201214_IMR90_9day','20201208_IMR90_3day']\n",
    "\n",
    "# selected_conditions = ['20201214_IMR90_9day','20201208_IMR90_3day']\n",
    "\n",
    "df = df[df.condition.apply(lambda c: c in selected_conditions)]\n",
    "df['condition'] = df.condition.apply(lambda c: 'old' if 'old' in c else 'young')\n",
    "\n",
    "if 'classification_manual' in df.columns:\n",
    "    feats = df.drop(['filename', 'dataset_name', 'condition', 'classification_manual', 'classification_auto'], 1).values\n",
    "else:\n",
    "    feats = df.drop(['filename', 'dataset_name', 'condition'], 1).values\n",
    "\n",
    "conditions = [f.split('/')[-3] for f in df.filename]\n",
    "\n",
    "replicates = [f.split('/')[-2] for f in df.filename]\n",
    "\n",
    "# we have some NaNs, impute them\n",
    "feats = Imputer().fit_transform(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# np.unique(replicates)\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: sort images into good/bad by features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('C:/Users/hoerl/Downloads/sorting20210316.json', 'r') as fd:\n",
    "    sorting_dict = json.load(fd)\n",
    "sorting_dict\n",
    "\n",
    "def get_classification_from_dict(row, sorting_dict):\n",
    "    filename = os.path.split(row.filename)[1].replace('.h5', '')\n",
    "    dataset_name = row.dataset_name\n",
    "\n",
    "    if [filename, dataset_name] in sorting_dict['good']:\n",
    "        return 'good'\n",
    "    elif [filename, dataset_name] in sorting_dict['bad']:\n",
    "        return 'bad'\n",
    "    else:\n",
    "        return 'unclassified'\n",
    "\n",
    "df['classification_manual'] = df.apply(lambda row: get_classification_from_dict(row, sorting_dict), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "has_goodbad = df.classification_manual.apply(lambda r: r in ['good', 'bad']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "Xs = StandardScaler().fit_transform(feats)\n",
    "ys = LabelEncoder().fit_transform(conditions)\n",
    "\n",
    "Xs_goodbad = Xs[has_goodbad]\n",
    "le_goodbad = LabelEncoder()\n",
    "ys_goodbad = le_goodbad.fit_transform(df.classification_manual.values[has_goodbad])\n",
    "# ys = LabelEncoder().fit_transform([a+b for a,b in zip(conditions,replicates)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# model = LogisticRegression(max_iter=1000)\n",
    "# model = SVC()\n",
    "# model = AdaBoostClassifier()\n",
    "# model = RandomForestClassifier()\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "np.mean(cross_val_score(model, Xs_goodbad, ys_goodbad))\n",
    "# np.mean(cross_val_score(model, pca.transform(Xs)[:,:20], ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_goodbad = DecisionTreeClassifier()\n",
    "model_goodbad.fit(Xs_goodbad, ys_goodbad)\n",
    "df['classification_auto'] = le_goodbad.inverse_transform(model_goodbad.predict(Xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# look at the number of good/bad images per condition\n",
    "df.groupby('condition').classification_auto.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('condition').perc_low.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save table with classification (e.g. for plotting of good/bad images)\n",
    "# df.to_csv('/scratch/hoerl/auto_sir_dna_comp/20210316_glcm_all_plusolddata_extrafeats_with_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV classifier on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "feats_to_drop = ['img_height', 'img_width',# 'intensity_mu', # maybe confounding\n",
    "                 'intensity_sigma',# 'mask_area', 'perc_high', 'perc_low',\n",
    "                 'classification_manual', 'classification_auto', # classification-related\n",
    "                 'filename', 'dataset_name', 'condition' ]\n",
    "\n",
    "feats = df.drop(feats_to_drop, 1).values\n",
    "feats = Imputer().fit_transform(feats)\n",
    "\n",
    "Xs = StandardScaler().fit_transform(feats[df.classification_auto == 'good'])\n",
    "ys = LabelEncoder().fit_transform(df.condition)[df.classification_auto == 'good']\n",
    "\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# Xs_p = PolynomialFeatures(2).fit_transform(Xs) # NOTE: using this is super slow -> PCA before cls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(feats_to_drop, 1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA().fit(Xs)\n",
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "# model = SVC()\n",
    "# model = AdaBoostClassifier()\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "np.mean(cross_val_score(model, Xs, ys))\n",
    "# np.mean(cross_val_score(model, pca.transform(Xs)[:,:100], ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy',\n",
    "              min_features_to_select=min_features_to_select)\n",
    "rfecv.fit(Xs, ys)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(min_features_to_select,\n",
    "               len(rfecv.grid_scores_) + min_features_to_select),\n",
    "         rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = df.drop(feats_to_drop, 1).values[:,rfecv.support_]\n",
    "Xs = StandardScaler().fit_transform(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tSNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# also select only good from condition labels\n",
    "# only necessary for plot\n",
    "conditions = [f.split('/')[-3] for f in df[df.classification_auto == 'good'].filename]\n",
    "replicates = [f.split('/')[-2] for f in df[df.classification_auto == 'good'].filename]\n",
    "conditions = df.condition\n",
    "np.unique(replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FastICA, FactorAnalysis\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(perplexity=30)\n",
    "\n",
    "# p = tsne.fit_transform(pca.transform(Xs)[:,:20])\n",
    "p = tsne.fit_transform(Xs)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.scatterplot(x=p.T[0], y=p.T[1], hue=conditions, alpha=1, s=60, palette=sns.color_palette('husl', len(np.unique(conditions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "sns.scatterplot(x=p.T[0], y=p.T[1], hue=conditions, style=replicates, s=60, palette=sns.color_palette('hls', len(np.unique(conditions))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.2)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fpr, tpr, thres = roc_curve(y_test, model.decision_function(X_test))\n",
    "plt.plot(fpr, tpr)\n",
    "auc(fpr, tpr)\n",
    "\n",
    "# np.round(model.predict_proba(X_test), 3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
