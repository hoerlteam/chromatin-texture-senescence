{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from pathlib2 import Path\n",
    "\n",
    "import numpy as np\n",
    "from numpy import interp\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneGroupOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import auc, average_precision_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load Features from skimage GLCM or CellProfiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) From skimage pipeline output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '/scratch/hoerl/auto_sir_dna_comp/20220829_glcm_good95_imagenorm.csv'\n",
    "# csv_file = '/scratch/hoerl/auto_sir_dna_comp/20220829_glcm_good95_imagenorm_confocalblur.csv'\n",
    "csv_file = '/scratch/hoerl/auto_sir_dna_comp/20220829_glcm_good95_replicatenorm.csv'\n",
    "# csv_file = '/scratch/hoerl/auto_sir_dna_comp/20220829_glcm_good95_replicatenorm_confocalblur.csv'\n",
    "\n",
    "save_plots = False\n",
    "\n",
    "# use the date of the csv file for saving plots\n",
    "date_str = Path(csv_file).name.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skimage = pd.read_csv(csv_file)\n",
    "\n",
    "### Optional 1: filter by foreground brightness\n",
    "# df_skimage = df_skimage[df_skimage.fg_mean > 100]\n",
    "\n",
    "### Optional 2: keep only replicates abov certain mean (mean) intensity\n",
    "# keep only replicates above a certain mean fg intensity\n",
    "only_bright = False\n",
    "if only_bright:\n",
    "    df_skimage = reduce(pd.DataFrame.append, [dfi for _,dfi in df_skimage.groupby(['cell_class', 'replicate']) if dfi.fg_mean.mean() > 100])\n",
    "\n",
    "### Optional 3: only use technical replicates with a minimum amount of images\n",
    "min_replicate_size = 10\n",
    "df_skimage = reduce(pd.DataFrame.append, [dfi for _, dfi in df_skimage.groupby(['cell_class', 'replicate']) if len(dfi)>min_replicate_size])\n",
    "\n",
    "### Optional 4: Group all ICM treated cells into one class\n",
    "# df_skimage.cell_class.replace(['IMR90_3d_ICM_young', 'IMR90_6d_ICM_young', 'IMR90_9d_ICM_young'], 'IMR90_ICM_treated', inplace=True)\n",
    "\n",
    "### 5: select only some cell_classes\n",
    "# selected_cell_classes = ['IMR90_untreated_old', 'IMR90_9d_ICM_young', 'IMR90_3d_ICM_young', 'IMR90_6d_ICM_young', 'IMR90_young_untreated']\n",
    "selected_cell_classes = ['IMR90_untreated_old', 'IMR90_6d_ICM_young', 'IMR90_young_untreated']\n",
    "# selected_cell_classes = ['IMR90_untreated_old', 'IMR90_young_untreated']\n",
    "\n",
    "# NOTE: only works with grouping of the treated cells\n",
    "# selected_cell_classes = ['IMR90_untreated_old', 'IMR90_young_untreated', 'IMR90_ICM_treated']\n",
    "\n",
    "df_skimage = df_skimage[df_skimage.cell_class.isin(selected_cell_classes)]\n",
    "\n",
    "# Get data 'batch' from preparation date\n",
    "# NOTE: scaling per batch (below) was not really helpfull\n",
    "dates = df_skimage.replicate.str.split('_', expand=True)[0]\n",
    "\n",
    "date_to_batch = {\n",
    "    '20200622' : 0,\n",
    "    '20200625' : 0,\n",
    "    '20200629' : 0,\n",
    "    '20200702' : 0,\n",
    "    '20200705' : 0,\n",
    "    '20201208' : 1,\n",
    "    '20201214' : 1,\n",
    "    '20210326' : 2,\n",
    "    '20210402' : 2,\n",
    "    '20210826' : 3,\n",
    "    '20211006' : 4,\n",
    "    '20220107' : 5,\n",
    "    '20220111' : 5\n",
    "}\n",
    "\n",
    "batches = dates.replace(date_to_batch)\n",
    "\n",
    "# columns to drop from features\n",
    "# filepaths, classes, good/bad cls & auxillariy features\n",
    "columns_to_drop = ['dataset_name', 'filename', 'classification_manual', 'classification_auto', 'replicate',\n",
    "                   'cell_class', 'condition',\n",
    "                   'img_height', 'img_width', 'mask_area',\n",
    "                   'num_blank_rows', 'num_blank_cols',\n",
    "#                    'intensity_mu', 'intensity_sigma', \n",
    "                   'perc_high', 'perc_low', 'fg_mean',\n",
    "                   'perc_high_image', 'perc_low_image'\n",
    "                  ] \n",
    "\n",
    "# drop columns that are not features\n",
    "df_feats = df_skimage.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "conditions = df_skimage.cell_class\n",
    "replicates = df_skimage.replicate\n",
    "bio_replicates = df_skimage.condition\n",
    "\n",
    "tex_values = df_feats.values\n",
    "feat_names = df_feats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = df_skimage.groupby(['cell_class', 'condition']).fg_mean.describe()[['count', 'mean', '50%']]\n",
    "r['bigger_than_100'] = r['mean'] > 100\n",
    "r['count'] = r['count'].astype(int)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Old version from CellProfiler\n",
    "\n",
    "NOTE: Not tested with recent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CellProfiler output\n",
    "obj_file = '/Volumes/davidh-ssd/examples_tiff_n_200_optimal_intensity_8bit_repl/MyExpt_IdentifyPrimaryObjects.csv'\n",
    "img_file = '/Volumes/davidh-ssd/examples_tiff_n_200_optimal_intensity_8bit_repl/MyExpt_Image.csv'\n",
    "\n",
    "obj_df = pd.read_csv(obj_file)\n",
    "img_df = pd.read_csv(img_file)\n",
    "\n",
    "# fix win pathnames\n",
    "img_df.PathName_DNA = img_df.PathName_DNA.str.replace('\\\\', '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = img_df.PathName_DNA.values[obj_df.ImageNumber.values - 1]\n",
    "folders = [p.split('/')[-1] for p in paths]\n",
    "\n",
    "# get condition/replicate from folder name\n",
    "conditions = ['_'.join(p.split('_')[1:-1]) for p in folders]\n",
    "replicates = ['_'.join([p.split('_')[0], p.split('_')[-1]]) for p in folders]\n",
    "\n",
    "# fix two different namings for \"old_untreated\"\n",
    "conditions = ['IMR90_untreated_old' if c == 'IMR90_old' else c for c in conditions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([conditions, replicates], axis=1, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features from object df\n",
    "# we comment out RadialDistribution & AreaShape, as we are not really interested\n",
    "# nucleus shape change or changes to globel chromatin localization\n",
    "feat_names = [c for c in obj_df.columns if c.startswith('Texture') \n",
    "#               or c.startswith('RadialDistribution')\n",
    "              or c.startswith('Granularity')\n",
    "               or c.startswith('Intensity')\n",
    "#               or c.startswith('AreaShape')\n",
    "              and not 'NormalizedMoment' in c and not 'EulerNumber' in c]\n",
    "\n",
    "# drop unwanted features\n",
    "tex_values = obj_df[feat_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for verification\n",
    "feat_names, tex_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric labels, scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "le = LabelEncoder()\n",
    "ys = le.fit_transform(conditions)\n",
    "\n",
    "# make one-hot encoded ys for OvR classification\n",
    "binarizer = LabelBinarizer()\n",
    "onehot_y = binarizer.fit_transform(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have NaNs -> impute\n",
    "tex_values = SimpleImputer().fit_transform(tex_values)\n",
    "\n",
    "# normalize features\n",
    "scaler = StandardScaler()\n",
    "tex_values = scaler.fit_transform(tex_values)\n",
    "\n",
    "# normalize per 'batch'\n",
    "# for b in np.unique(batches):\n",
    "#     mask = [bi == b for bi in batches]\n",
    "#     tex_values[mask] = StandardScaler().fit_transform(tex_values[mask])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Feature heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repl_for_plot = replicates\n",
    "repl_for_plot = bio_replicates\n",
    "\n",
    "idxs = [idx for idx, _ in sorted(enumerate(zip(conditions, repl_for_plot)), key=lambda x: x[1])]\n",
    "\n",
    "ylabs = {}\n",
    "for comb in sorted(set(zip(conditions, repl_for_plot))):\n",
    "    idx = sorted(zip(conditions, repl_for_plot)).index(comb)\n",
    "    ylabs[', '.join(comb) + ' (â†“)'] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (15, 15)\n",
    "\n",
    "# get an aspect ration (cols/rows) so that we reach roughly the desired figure size\n",
    "aspect_to_fit_figsize = np.divide(*(np.array(figsize[::-1]) / tex_values.shape))\n",
    "\n",
    "# optional: manual row stretch\n",
    "row_stretch = 1.0\n",
    "aspect = aspect_to_fit_figsize * row_stretch\n",
    "\n",
    "\n",
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(tex_values[idxs].clip(*np.quantile(tex_values, [0.01, 0.99])),\n",
    "           aspect=aspect, cmap='coolwarm', interpolation='nearest')\n",
    "\n",
    "plt.yticks(list(ylabs.values()), list(ylabs.keys()));\n",
    "plt.xticks(np.arange(len(feat_names)), feat_names, rotation='vertical');\n",
    "plt.title('Features');\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "plt.colorbar(shrink=.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(f'/scratch/hoerl/auto_sir_dna_comp/{date_str}_heatmap_{\"bright\" if only_bright else \"all\"}replicates.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use Feature Aggregation to produce a heatmap of aggregate features\n",
    "\n",
    "NOTE: not used in classification below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (15, 15)\n",
    "n_aggregated_features = 20\n",
    "\n",
    "# get feature aggregator and aggregate texture feats\n",
    "agg = FeatureAgglomeration(n_clusters=n_aggregated_features)\n",
    "tex_values_agg = agg.fit_transform(tex_values)\n",
    "\n",
    "# get an aspect ration (cols/rows) so that we reach roughly the desired figure size\n",
    "aspect_to_fit_figsize = np.divide(*(np.array(figsize[::-1]) / tex_values_agg.shape))\n",
    "\n",
    "# optional: manual row stretch\n",
    "row_stretch = 1.0\n",
    "aspect = aspect_to_fit_figsize * row_stretch\n",
    "\n",
    "plt.figure(figsize = figsize)\n",
    "plt.imshow(tex_values_agg[idxs].clip(*np.quantile(tex_values_agg, [0.01, 0.99])),\n",
    "           aspect=aspect, cmap='coolwarm', interpolation='nearest')\n",
    "\n",
    "plt.yticks(list(ylabs.values()), list(ylabs.keys()));\n",
    "plt.title('Aggregated Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) tSNE embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate tsne\n",
    "ts = TSNE(perplexity=100, init='pca', learning_rate='auto').fit_transform(tex_values)\n",
    "\n",
    "# alternative: just do PCA\n",
    "# ts = PCA().fit_transform(tex_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('bright')\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "sns.scatterplot(x=ts.T[0], y=ts.T[1], hue=conditions, alpha=0.35, s=25)\n",
    "# sns.scatterplot(x=ts.T[0], y=ts.T[1], hue=conditions, alpha=0.65, size=df_skimage.fg_mean)\n",
    "plt.xlabel('tSNE comp. 1'); plt.ylabel('tSNE comp. 2');\n",
    "plt.legend(fontsize='large')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "if save_plots:\n",
    "    plt.savefig(f'/scratch/hoerl/auto_sir_dna_comp/{date_str}_tsne_all_{\"bright\" if only_bright else \"all\"}replicates.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_for_plot = replicates\n",
    "# repl_for_plot = bio_replicates\n",
    "\n",
    "for condition in np.unique(conditions):\n",
    "    \n",
    "    # label: replicate for current class, \"others\" for all other points\n",
    "    repl_label = [r if c==condition else 'others' for r, c in zip(repl_for_plot, conditions)]\n",
    "\n",
    "    # color others gray, replicates in different colors\n",
    "    cm_ = get_cmap('rainbow', len(np.unique(repl_label))-1)\n",
    "    sns.set_palette(sns.color_palette(['lightgray'] + [cm_(i) for i in range(len(np.unique(repl_label))-1)]))\n",
    "    \n",
    "    # sort so we plot \"others\" first\n",
    "    ts_ = ts[np.argsort(repl_label)[::-1]]\n",
    "    repl_label.sort(reverse=True)\n",
    "      \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(x=ts_.T[0], y=ts_.T[1], hue=repl_label,\n",
    "                    alpha=0.65, s=25)\n",
    "    \n",
    "    plt.title(condition);\n",
    "    plt.xlabel('tSNE comp. 1'); plt.ylabel('tSNE comp. 2');\n",
    "    plt.legend(fontsize='large')\n",
    "    \n",
    "    plt.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(f'/scratch/hoerl/auto_sir_dna_comp/{date_str}_tsne_{condition}_{\"bright\" if only_bright else \"all\"}replicates.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot example images from a range of tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py as h5\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def load_single_image_from_h5(filename, dataset_name, norm_quantiles):\n",
    "    with h5.File(filename, 'r') as fd:\n",
    "        img = fd[f'/experiment/{dataset_name}/0/0'][...].squeeze()\n",
    "    return rescale_intensity(img.astype(np.float32), in_range=norm_quantiles)\n",
    "\n",
    "# min_t = np.array([-40, 0])\n",
    "# max_t = np.array([-20, 20])\n",
    "\n",
    "# min_t = np.array([-20, -20])\n",
    "# max_t = np.array([0, -10])\n",
    "\n",
    "# min_t = np.array([30, -10])\n",
    "# max_t = np.array([40, 0])\n",
    "\n",
    "min_t = np.array([45, 0])\n",
    "max_t = np.array([55, 20])\n",
    "\n",
    "n_images_per_class = 4\n",
    "\n",
    "selection = np.all(ts > min_t, axis=1) & np.all(ts < max_t, axis=1)\n",
    "\n",
    "df_tsne_range = df_skimage[selection]\n",
    "\n",
    "imgs = defaultdict(list)\n",
    "for cell_cls, dfi in df_tsne_range.groupby('cell_class'):\n",
    "    sample = dfi[['filename', 'dataset_name', 'perc_low', 'perc_high']].sample(min(n_images_per_class, len(dfi)))\n",
    "    for _, (filename, dataset_name, perc_low, perc_high) in sample.iterrows():\n",
    "        imgs[cell_cls].append(load_single_image_from_h5(filename, dataset_name, (perc_low, perc_high)))\n",
    "    \n",
    "    \n",
    "fig, axs = plt.subplots(ncols=n_images_per_class, nrows=len(imgs), figsize=(16,12))\n",
    "for (cl, imgs_cls), axs_cls in zip(imgs.items(), axs):\n",
    "    for img, ax in zip(imgs_cls, axs_cls):\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(cl)\n",
    "        ax.axis('off')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) cross-val and PR curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (8, 8)\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "# cls = SVC(C=100.0, probability=True, class_weight='balanced')\n",
    "cls = SVC(C=0.01, probability=True, class_weight='balanced')\n",
    "# cls = RandomForestClassifier(300, class_weight='balanced')\n",
    "\n",
    "cv_strategies = (\n",
    "    (StratifiedKFold(10, shuffle=True), None, '10-Fold Stratified CV'),\n",
    "    (LeaveOneGroupOut(), [c + '_' + r for c,r in zip(conditions, replicates)], 'Leave-One-Replicate-Out (Technical)'),\n",
    "    (LeaveOneGroupOut(), [c + '_' + r for c,r in zip(conditions, bio_replicates)], 'Leave-One-Replicate-Out (Biological)')\n",
    ")\n",
    "\n",
    "for cv, groups, desc in cv_strategies:\n",
    "    cv.split(tex_values, ys, groups)\n",
    "\n",
    "    prob_pred = cross_val_predict(cls, tex_values, ys, cv=cv, groups=groups, n_jobs=-1, method='predict_proba')\n",
    "    pred = np.argmax(prob_pred, axis=1)\n",
    "\n",
    "    # we have only 2 classes -> use probabilities for class 1 for further analysis\n",
    "    if prob_pred.shape[1] == 2:\n",
    "        prob_pred = prob_pred[:,1:2] # 1-column selection\n",
    "\n",
    "    # get accuracy, AP & PR curve\n",
    "    overall_acc = (pred == ys).sum() / len(ys)\n",
    "    pr, re, _ = precision_recall_curve(onehot_y.ravel(), prob_pred.ravel())\n",
    "    ap = average_precision_score(onehot_y.ravel(), prob_pred.ravel())\n",
    "\n",
    "    ax.plot(re, pr, label=f'{desc}\\naccuracy={overall_acc:.3f}\\nAP={ap:.3f}', lw=2)\n",
    "\n",
    "# add \"random guess\" baseline to all plots\n",
    "_, cts = np.unique(ys, return_counts=True)\n",
    "baseline_pr = np.mean(cts/len(ys))\n",
    "\n",
    "ax.plot([0, 1], [baseline_pr, baseline_pr], linestyle='dashed', color='firebrick', lw=2, label='random guess')\n",
    "    \n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cross-val prediction and string labels\n",
    "\n",
    "cv, groups, _ = cv_strategies[2]\n",
    "\n",
    "pred = cross_val_predict(cls, tex_values, ys, cv=cv, n_jobs=-1, groups=groups)\n",
    "labs_pred = le.inverse_transform(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = defaultdict(lambda : np.zeros(np.max(ys) + 1))\n",
    "\n",
    "# replicates_ = replicates\n",
    "replicates_ = bio_replicates\n",
    "\n",
    "# go through all predictions, increment corresponding row\n",
    "for cond, repl, lab_pred in zip(conditions, replicates_, labs_pred):\n",
    "    conf_mat[(cond, repl)][le.transform([lab_pred])[0]] += 1\n",
    "\n",
    "# get sorted label + number of samples\n",
    "input_cls = [s[0] + (f'N: {int(s[1].sum())}' ,) for s in sorted(conf_mat.items())]\n",
    "\n",
    "# make matrix from dict, normalize per-row\n",
    "mat = np.array([s[1] for s in sorted(conf_mat.items())])\n",
    "mat = mat / np.sum(mat, axis=1).reshape((-1,1))\n",
    "\n",
    "# plot as heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(mat, cmap='Blues', aspect=0.2)\n",
    "plt.yticks(ticks=np.arange(len(input_cls)), labels=[', '.join(c) for c in input_cls]);\n",
    "plt.xticks(ticks=np.arange(np.max(ys) + 1), labels=le.inverse_transform(np.arange(np.max(ys) + 1)), rotation='vertical');\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.colorbar(shrink=.8)\n",
    "if save_plots:\n",
    "    plt.savefig(f'/scratch/hoerl/auto_sir_dna_comp/{date_str}_confusionmatrix_{\"bright\" if only_bright else \"all\"}replicates.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Cross-Validation Code and plots per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier with optional feature agglomeration\n",
    "# cls = Pipeline([('cluster_feat', FeatureAgglomeration(n_clusters=50)), ('cls', RandomForestClassifier(n_estimators=400, class_weight='balanced'))])\n",
    "# cls = Pipeline([('cls', LogisticRegression(max_iter=1000))])\n",
    "# cls = Pipeline([('cls', SVC(probability=True))])\n",
    "# cls = Pipeline([('cls', RandomForestClassifier(n_estimators=300))])\n",
    "\n",
    "# set up cross-val split strategy\n",
    "cv = StratifiedKFold(5, shuffle=True)\n",
    "groups = None\n",
    "\n",
    "# cv = LeaveOneGroupOut()\n",
    "# groups = LabelEncoder().fit_transform(bio_replicates)\n",
    "\n",
    "# cls = RandomForestClassifier(n_estimators=300)\n",
    "cls = SVC(C=100.0, probability=True)\n",
    "\n",
    "# NB: for cross_val_score, use the non-onehot ys\n",
    "# cross_val_score(ovr_cls, tex_values, ys, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_perclass = defaultdict(list)\n",
    "res_avg = defaultdict(list)\n",
    "\n",
    "prob_pred = cross_val_predict(cls, tex_values, ys, cv=cv, groups=groups, n_jobs=-1, method='predict_proba')\n",
    "pred = np.argmax(prob_pred, axis=1)\n",
    "\n",
    "# we have only 2 classes -> use probabilities for class 1 for further analysis\n",
    "if prob_pred.shape[1] == 2:\n",
    "    prob_pred = prob_pred[:,1:2] # 1-column selection\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(tex_values, ys, groups=groups)):\n",
    "       \n",
    "    # get overall multiclass accuracy    \n",
    "    overall_acc = (pred[test] == ys[test]).sum() / len(test)\n",
    "    \n",
    "    # get PR / ROC metrics for each class\n",
    "    for j in range(prob_pred[test].shape[1]):\n",
    "        \n",
    "        # NB: in 2-class case, onehot_y will be just y\n",
    "        fpr, tpr, _ = roc_curve(onehot_y[test, j], prob_pred[test, j])\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        pr, re, _ = precision_recall_curve(onehot_y[test, j], prob_pred[test, j])\n",
    "        ap = average_precision_score(onehot_y[test, j], prob_pred[test, j])\n",
    "        \n",
    "        res_perclass[(j, 'pr')].append(pr)\n",
    "        res_perclass[(j, 're')].append(re)\n",
    "        res_perclass[(j, 'fpr')].append(fpr)\n",
    "        res_perclass[(j, 'tpr')].append(tpr)\n",
    "        res_perclass[(j, 'auc')].append(auc_roc)\n",
    "        res_perclass[(j, 'ap')].append(ap)\n",
    "        \n",
    "\n",
    "    # get PR / ROC micro-average\n",
    "    fpr, tpr, _ = roc_curve(onehot_y[test].ravel(), prob_pred[test].ravel())\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    \n",
    "    pr, re, _ = precision_recall_curve(onehot_y[test].ravel(), prob_pred[test].ravel())\n",
    "    ap = average_precision_score(onehot_y[test].ravel(), prob_pred[test].ravel())\n",
    "        \n",
    "    res_avg['acc'].append(overall_acc)\n",
    "    res_avg['pr'].append(pr)\n",
    "    res_avg['re'].append(re)\n",
    "    res_avg['fpr'].append(fpr)\n",
    "    res_avg['tpr'].append(tpr)\n",
    "    res_avg['auc'].append(auc_roc)\n",
    "    res_avg['ap'].append(ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC + PR curves per class / CV-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plots, cmap\n",
    "cmap = get_cmap('Set3', np.max(ys)+1)\n",
    "fig_pr, axs_pr = plt.subplots(ncols=2, nrows=1, figsize=(20,8))\n",
    "fig_roc, axs_roc = plt.subplots(ncols=2, nrows=1, figsize=(20,8))\n",
    "\n",
    "\n",
    "# add \"random guess\" baseline to all plots\n",
    "_, cts = np.unique(ys, return_counts=True)\n",
    "baseline_pr = np.mean(cts/len(ys))\n",
    "\n",
    "axs_pr[0].plot([0, 1], [baseline_pr, baseline_pr], linestyle='dashed', color='gray', lw=3, label='random guess')\n",
    "axs_pr[1].plot([0, 1], [baseline_pr, baseline_pr], linestyle='dashed', color='gray', lw=3, label='random guess')\n",
    "axs_pr[0].plot([0, 1, 1], [1, 1, 0], linestyle='dashed', color='darkgreen', lw=3, label='perfect classifier')\n",
    "axs_pr[1].plot([0, 1, 1], [1, 1, 0], linestyle='dashed', color='darkgreen', lw=3, label='perfect classifier')\n",
    "\n",
    "axs_roc[0].plot([0,1], [0,1], linestyle='dashed', color='gray', lw=3, label='random guess')\n",
    "axs_roc[1].plot([0,1], [0,1], linestyle='dashed', color='gray', lw=3, label='random guess')\n",
    "axs_roc[0].plot([0, 0, 1], [0, 1, 1], linestyle='dashed', color='darkgreen', lw=3, label='perfect classifier')\n",
    "axs_roc[1].plot([0, 0, 1], [0, 1, 1], linestyle='dashed', color='darkgreen', lw=3, label='perfect classifier')\n",
    "\n",
    "\n",
    "# curves for micro-averages\n",
    "x_range = np.linspace(0, 1, 200)\n",
    "tprs_interp = []\n",
    "prs_interp = []\n",
    "for tpr, fpr, pr, re in zip(res_avg['tpr'], res_avg['fpr'], res_avg['pr'], res_avg['re']):\n",
    "    \n",
    "    # plot raw curves for split\n",
    "    axs_pr[1].plot(re, pr, color='steelblue', alpha=0.5)   \n",
    "    axs_roc[1].plot(fpr, tpr, color='steelblue', alpha=0.5)\n",
    "    \n",
    "    # interpolate curves at defined x locations\n",
    "    tprs_interp.append(interp(x_range, fpr, tpr))\n",
    "    # NB: invert re, pr as they start with recall 1, but we go from 0 to 1\n",
    "    prs_interp.append(interp(x_range, re[::-1], pr[::-1]))\n",
    "\n",
    "# average interpolated curves\n",
    "tpr_mean = np.mean(np.array(tprs_interp), axis=0)\n",
    "pr_mean = np.mean(np.array(prs_interp), axis=0)\n",
    "\n",
    "# plot interpolated curves + some info\n",
    "label_roc = f'''micro-average AUC: {np.round(np.mean(res_avg[\"auc\"]), 2)} +- {np.round(np.std(res_avg[\"auc\"]), 2)}\n",
    "accuracy: {np.round(np.mean(res_avg[\"acc\"]), 2)} +- {np.round(np.std(res_avg[\"auc\"]), 2)}'''\n",
    "label_pr = f'''micro-average AP: {np.round(np.mean(res_avg[\"ap\"]), 2)} +- {np.round(np.std(res_avg[\"ap\"]), 2)}\n",
    "accuracy: {np.round(np.mean(res_avg[\"acc\"]), 2)} +- {np.round(np.std(res_avg[\"auc\"]), 2)}'''\n",
    "axs_roc[1].plot(x_range, tpr_mean, color='steelblue', lw=3,\n",
    "                label=label_roc)\n",
    "axs_pr[1].plot(x_range, pr_mean, color='steelblue', lw=3,\n",
    "                label=label_pr)\n",
    "\n",
    "\n",
    "# curves per class\n",
    "# similar to average above\n",
    "for j in range(np.max(ys)+1 if np.max(ys)>1 else 1):\n",
    "    x_range = np.linspace(0, 1, 200)\n",
    "    tprs_interp = []\n",
    "    prs_interp = []\n",
    "    for tpr, fpr, pr, re in zip(res_perclass[(j, 'tpr')], res_perclass[(j, 'fpr')],\n",
    "                                res_perclass[(j, 'pr')], res_perclass[(j, 're')]):\n",
    "        tprs_interp.append(interp(x_range, fpr, tpr))\n",
    "        prs_interp.append(interp(x_range, re[::-1], pr[::-1]))\n",
    "        \n",
    "        axs_roc[0].plot(fpr, tpr, color=cmap(j), alpha=0.5)\n",
    "        axs_pr[0].plot(re, pr, color=cmap(j), alpha=0.5)\n",
    "        \n",
    "    tpr_mean = np.mean(np.array(tprs_interp), axis=0)\n",
    "    pr_mean = np.mean(np.array(prs_interp), axis=0)\n",
    "\n",
    "    axs_roc[0].plot(x_range, tpr_mean, lw=3, color=cmap(j),\n",
    "                    label=f'{le.inverse_transform([j])[0]} AUC: {np.round(np.mean(res_perclass[(j, \"auc\")]), 2)} +- {np.round(np.std(res_perclass[(j,\"auc\")]), 2)}')\n",
    "    axs_pr[0].plot(x_range, pr_mean, lw=3, color=cmap(j),\n",
    "                    label=f'{le.inverse_transform([j])[0]} AP: {np.round(np.mean(res_perclass[(j,\"ap\")]), 2)} +- {np.round(np.std(res_perclass[(j,\"ap\")]), 2)}')\n",
    "\n",
    "    \n",
    "# labels, legends, etc.\n",
    "axs_pr[0].set_title('PR curve (per class)')\n",
    "axs_pr[0].set_xlabel('Recall')\n",
    "axs_pr[0].set_ylabel('Precision')\n",
    "axs_pr[1].set_title('PR curve (micro-average)')\n",
    "axs_pr[1].set_xlabel('Recall')\n",
    "axs_pr[1].set_ylabel('Precision')\n",
    "\n",
    "axs_roc[0].set_title('ROC curve (per class)')\n",
    "axs_roc[0].set_xlabel('False Positive Rate')\n",
    "axs_roc[0].set_ylabel('True Positive Rate')\n",
    "axs_roc[1].set_title('ROC curve (micro-average)')\n",
    "axs_roc[1].set_xlabel('False Positive Rate')\n",
    "axs_roc[1].set_ylabel('True Positive Rate')\n",
    "    \n",
    "axs_roc[0].legend(fontsize='large')\n",
    "axs_pr[0].legend(fontsize='large')\n",
    "axs_roc[1].legend(fontsize='large')\n",
    "axs_pr[1].legend(fontsize='large')\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "if save_plots:\n",
    "    fig_pr.savefig(f'/scratch/hoerl/auto_sir_dna_comp/{date_str}_prcurves_{\"bright\" if only_bright else \"all\"}replicates.pdf', transparent=True)\n",
    "    fig_roc.savefig(f'/scratch/hoerl/auto_sir_dna_comp/{date_str}_roccurves_{\"bright\" if only_bright else \"all\"}replicates.pdf', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD: Grid search hyperparameters\n",
    "\n",
    "Requires cls to be a pipeline of Feature aggregation followed by classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cluster_grid = np.arange(10, len(feat_names), 10)\n",
    "n_estimators_grid = np.arange(100, 401, 50)\n",
    "n_cluster_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(cls, {\n",
    "    'cluster_feat__n_clusters' : n_cluster_grid,\n",
    "    'cls__n_estimators': n_estimators_grid},\n",
    "                  cv=cv, n_jobs=-1)\n",
    "gs.fit(tex_values, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(gs.best_estimator_, tex_values, ys, cv=cv)\n",
    "cv_scores.mean(), cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set classifier to use to best result from CV\n",
    "cls = gs.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
