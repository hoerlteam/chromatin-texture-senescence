{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0eed1c-aff2-4893-ba97-ea1978ca2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import random\n",
    "from itertools import product\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.base import clone\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159be914-2b4a-4981-9a7b-70cc07a049e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220125_glcm_good_lithreshold_smallblur.csv')\n",
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20211115_glcm_good_lithreshold_smallblur.csv')\n",
    "# df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20210719_glcm_good_all_confocalblur.csv')\n",
    "df = pd.read_csv('/scratch/hoerl/auto_sir_dna_comp/20220816_glcm_good_replicatenorm_confocalblur.csv')\n",
    "\n",
    "# rename 'IMR90_young' to 'IMR90_young_untreated'\n",
    "df.cell_class = df.cell_class.replace(['IMR90_young'], 'IMR90_young_untreated')\n",
    "\n",
    "# remove the \"young\" cells, as we do not have enough replicates with high mean intensity\n",
    "# df = df[~ df.cell_class.isin(['IMR90_young_untreated'])]\n",
    "\n",
    "# remove 3d, 9d -> we do not have enough biological replicates\n",
    "# df = df[~ df.cell_class.isin(['IMR90_3d_ICM_young', 'IMR90_9d_ICM_young'])]\n",
    "\n",
    "# keep only replicates above a certain mean fg intensity\n",
    "# df = reduce(pd.DataFrame.append, [dfi for _,dfi in df.groupby(['cell_class', 'replicate']) if dfi.fg_mean.mean() > 100])\n",
    "\n",
    "# give the replicates from last batch from 6-well extra 'w' suffix\n",
    "# idx = df.filename.str.contains('data') & df.filename.str.contains('well')\n",
    "# df.loc[idx, 'replicate'] += 'w'\n",
    "\n",
    "# Optional: filter by foreground brightness\n",
    "# df = df[df.fg_mean > 100]\n",
    "\n",
    "# filter replicates with small number of cells\n",
    "min_num_cells = 10\n",
    "df = reduce(pd.DataFrame.append, [dfi for _, dfi in df.groupby(['cell_class', 'replicate']) if len(dfi) > min_num_cells])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1661e-bf09-4303-be6e-383dc1312b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['cell_class', 'condition']).fg_mean.describe()[['count', 'mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc7c894-fa4a-45cd-9bc8-8ab8b796df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition -> replicates dict\n",
    "d = {k: dfi.replicate.unique() for k, dfi in df.groupby(['cell_class'])}\n",
    "\n",
    "# get all possible ways of leaving one replicate per condition out\n",
    "combos = list(product(*d.values()))\n",
    "len(combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d1a51-875b-4fe2-bfe9-3c8b4433b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_split(df, combo, cls=SVC()):\n",
    "    '''\n",
    "    split df by using one replicate per cell_class as val set, the rest as train\n",
    "    train a classifier and return validation score    \n",
    "    '''\n",
    "    \n",
    "    # combo: list of (cell_class, replicate) tuples\n",
    "    df_val = reduce(pd.DataFrame.append, [dfi for i, dfi in df.groupby(['cell_class', 'replicate']) if i in combo])\n",
    "    df_train = reduce(pd.DataFrame.append, [dfi for i, dfi in df.groupby(['cell_class', 'replicate']) if i not in combo])\n",
    "\n",
    "    # cell_class is target\n",
    "    conditions = df_train.cell_class\n",
    "    conditions_val = df_val.cell_class\n",
    "    \n",
    "    # to numeric labels\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(conditions)\n",
    "    y_val = le.transform(conditions_val)\n",
    "\n",
    "    # get feature cols\n",
    "    columns_to_drop = ['dataset_name', 'filename', 'classification_manual', 'classification_auto', 'replicate',\n",
    "                       'cell_class', 'condition', 'img_height', 'img_width', 'mask_area',\n",
    "#                       'intensity_mu','fg_mean', 'intensity_sigma', \n",
    "                       'perc_high', 'perc_low'\n",
    "                      ]  # + [c for c in df.columns if not 'LBP' in c ]\n",
    "\n",
    "    x_train = df_train.drop(columns_to_drop, 1).values\n",
    "    x_val = df_val.drop(columns_to_drop, 1).values\n",
    "\n",
    "    # impute NaNs and normalize\n",
    "    x_train = SimpleImputer().fit_transform(x_train)\n",
    "    x_val = SimpleImputer().fit_transform(x_val)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_val = sc.transform(x_val)\n",
    "\n",
    "    # train new cls and val\n",
    "    cls = clone(cls)\n",
    "    cls.fit(x_train, y_train)\n",
    "    score = cls.score(x_val, y_val)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe6849-3eff-4254-b1f1-ab4ca60d0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many random splits to try\n",
    "n_repeat = 200\n",
    "\n",
    "# classifier to use\n",
    "cls = RandomForestClassifier(300)\n",
    "\n",
    "# sample combos and handle multithreaded\n",
    "with ThreadPoolExecutor() as tpe:\n",
    "    \n",
    "    futures = []\n",
    "    combo_sample = random.sample(combos, n_repeat)\n",
    "    for combo in combo_sample:\n",
    "        # list of (cond, rep) to leave out\n",
    "        combo = list(zip(d.keys(), combo))\n",
    "        futures.append(tpe.submit(run_split, df, combo, cls))\n",
    "\n",
    "    # get results: scores\n",
    "    scores = []\n",
    "    for f in tqdm.tqdm(futures):\n",
    "        scores.append(f.result())\n",
    "        \n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87c5fe-e324-4bc0-a353-0acaab61ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score histogram\n",
    "plt.hist(scores, bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada5509c-4b49-4b79-9099-4c1811dbbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame.from_dict(dict(zip(d.keys(), np.array(combo_sample).T)))\n",
    "score_df['score'] = scores\n",
    "\n",
    "for di in d.keys():\n",
    "    print(score_df.groupby(di).score.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e3179-076f-4ece-9653-18df90db9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f017ead-d78a-4f37-89bb-8ef538db3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['cell_class', 'replicate']).fg_mean.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541f4bb-2472-44a4-93c2-c8932096dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2c4ba-3638-45f9-afef-06a8d09fe5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_predict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "group_columns = ['cell_class', 'replicate']\n",
    "class_columns = ['cell_class']\n",
    "\n",
    "cls = SVC(C=100.0, class_weight='balanced', probability=True)\n",
    "# cls = SVC(C=100.0, probability=True)\n",
    "# cls = RandomForestClassifier(n_estimators=300, class_weight='balanced')\n",
    "# cls = RandomForestClassifier()\n",
    "\n",
    "# NOTE: over/undersampling to balance classes did not really help...\n",
    "# cls = make_pipeline(\n",
    "#     RandomOverSampler(),    \n",
    "#     cls\n",
    "# )\n",
    "\n",
    "# columns to drop from features\n",
    "# filepaths, classes, good/bad cls & auxillariy features\n",
    "columns_to_drop = ['dataset_name', 'filename', 'classification_manual', 'classification_auto', 'replicate',\n",
    "                   'cell_class', 'condition',\n",
    "                   'img_height', 'img_width', 'mask_area',\n",
    "                   'num_blank_rows', 'num_blank_cols',\n",
    "#                    'intensity_mu', 'intensity_sigma', \n",
    "                   'perc_high', 'perc_low', 'fg_mean',\n",
    "                   'perc_high_image', 'perc_low_image'\n",
    "                  ] \n",
    "\n",
    "X = df.drop(columns = columns_to_drop).values\n",
    "X = SimpleImputer().fit_transform(X)\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[class_columns].values.ravel())\n",
    "\n",
    "groups = LabelEncoder().fit_transform(df[group_columns].apply(lambda r: reduce(add, r), axis=1))\n",
    "\n",
    "prob = cross_val_predict(cls, X, y, cv=LeaveOneGroupOut(), groups=groups, n_jobs=16, method='predict_proba')\n",
    "pred = np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e5efe-3197-49a8-8641-13cd77437b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(pred == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97531b4d-9fef-47f9-aea2-332c027072d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(X, y)\n",
    "cls.class_weight_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c1739-e42d-4e9b-a67d-6255cb747f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a71d5-a5a1-4134-8366-3d179484d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_columns = ['cell_class', 'condition']\n",
    "# label_columns = ['cell_class']\n",
    "\n",
    "conf_mat = defaultdict(lambda : np.zeros(np.max(y) + 1))\n",
    "\n",
    "# go through all predictions, increment corresponding row\n",
    "for lab_pred, *grp in zip(pred, *df[label_columns].values.T,):\n",
    "    conf_mat[tuple(grp)][lab_pred] += 1\n",
    "\n",
    "# get sorted label + number of samples\n",
    "input_cls = [s[0] + (f'N: {int(s[1].sum())}' ,) for s in sorted(conf_mat.items())]\n",
    "\n",
    "# make matrix from dict, normalize per-row\n",
    "mat = np.array([s[1] for s in sorted(conf_mat.items())])\n",
    "mat = mat / np.sum(mat, axis=1).reshape((-1,1))\n",
    "\n",
    "# plot as heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(mat, cmap='Blues', aspect=0.2)\n",
    "plt.yticks(ticks=np.arange(len(input_cls)), labels=[', '.join(c) for c in input_cls]);\n",
    "plt.xticks(ticks=np.arange(np.max(y) + 1), labels=le.inverse_transform(np.arange(np.max(y) + 1)), rotation='vertical');\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.colorbar(shrink=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebd3c8-aeab-4a45-90b7-228a23254d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_columns = ['cell_class', 'condition']\n",
    "# label_columns = ['cell_class']\n",
    "\n",
    "conf_mat = defaultdict(lambda : np.zeros(np.max(y) + 1))\n",
    "\n",
    "# go through all predictions, increment corresponding row\n",
    "for prob_, *grp in zip(prob, *df[label_columns].values.T,):\n",
    "    conf_mat[tuple(grp)] += prob_\n",
    "\n",
    "# get sorted label + number of samples\n",
    "input_cls = [s[0] + (f'N: {int(s[1].sum())}' ,) for s in sorted(conf_mat.items())]\n",
    "\n",
    "# make matrix from dict, normalize per-row\n",
    "mat = np.array([s[1] for s in sorted(conf_mat.items())])\n",
    "mat = mat / np.sum(mat, axis=1).reshape((-1,1))\n",
    "\n",
    "# plot as heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(mat, cmap='Blues', aspect=0.2)\n",
    "plt.yticks(ticks=np.arange(len(input_cls)), labels=[', '.join(c) for c in input_cls]);\n",
    "plt.xticks(ticks=np.arange(np.max(y) + 1), labels=le.inverse_transform(np.arange(np.max(y) + 1)), rotation='vertical');\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.colorbar(shrink=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef85ac6-ffcf-4a2b-951d-32d68049780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import comb\n",
    "\n",
    "comb(28, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de1b90-188f-4713-9f48-44cbe0a9ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, islice\n",
    "from math import comb\n",
    "\n",
    "\n",
    "def nth(iterable, n, default=None):\n",
    "    \"Returns the nth item or a default value\"\n",
    "    return next(islice(iterable, n, None), default)\n",
    "\n",
    "def sample_iterable(iterable, n, length):\n",
    "    idxs = random.sample(range(length), n)\n",
    "    print(idxs)\n",
    "    return [nth(iterable, idx) for idx in idxs]\n",
    "\n",
    "sample_iterable(combinations(prob, 2), 2, comb(len(prob), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d339f-8f72-4802-b146-af23bbef6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def sample_combinations(N, k, m):\n",
    "    \"\"\"\n",
    "    randomly sample m sets of indices to sample k elements from a length-N list/array\n",
    "    \"\"\"\n",
    "    res = set()\n",
    "    while(len(res) < m):\n",
    "        res.add(tuple(sorted(random.sample(range(N), k))))\n",
    "    return list(res)\n",
    "\n",
    "\n",
    "k_images = 5\n",
    "m_combos = 10000\n",
    "\n",
    "label_columns = ['cell_class', 'replicate']\n",
    "# label_columns = ['cell_class']\n",
    "\n",
    "conf_mat = defaultdict(lambda : np.zeros(np.max(y) + 1))\n",
    "\n",
    "df_ = df.copy()\n",
    "df_[[f'prob{i}' for i in range(prob.shape[1])]] = prob\n",
    "\n",
    "for c, dfi in df_.groupby(label_columns):\n",
    "    probs_i = dfi[[f'prob{i}' for i in range(prob.shape[1])]].values\n",
    "    \n",
    "    for s in sample_combinations(len(probs_i), k_images, m_combos):\n",
    "        pr_ = np.argmax(probs_i[list(s)].sum(axis=0))\n",
    "        conf_mat[tuple(c)][pr_] += 1\n",
    "        \n",
    "# get sorted label + number of samples\n",
    "input_cls = [s[0] + (f'N: {int(s[1].sum())}' ,) for s in sorted(conf_mat.items())]\n",
    "\n",
    "# make matrix from dict, normalize per-row\n",
    "mat = np.array([s[1] for s in sorted(conf_mat.items())])\n",
    "mat = mat / np.sum(mat, axis=1).reshape((-1,1))\n",
    "\n",
    "# plot as heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(mat, cmap='Blues', aspect=0.2)\n",
    "plt.yticks(ticks=np.arange(len(input_cls)), labels=[', '.join(c) for c in input_cls]);\n",
    "plt.xticks(ticks=np.arange(np.max(y) + 1), labels=le.inverse_transform(np.arange(np.max(y) + 1)), rotation='vertical');\n",
    "\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.colorbar(shrink=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643bf90-ba50-414e-81dd-afc2523dddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (cl, rep), labs in conf_mat.items():\n",
    "    print(cl, rep, labs[le.transform([cl])[0]] / labs.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ca07a-3a36-42ea-a580-1dcf78b015ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc386a2-7e15-4c2e-8c17-9c51229daa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "replicates = [gr for gr, _ in df.groupby(group_columns)]\n",
    "for i, replicate in enumerate(replicates):\n",
    "\n",
    "    # use one replicate as val, rest as train\n",
    "    df_val = df[(df[group_columns] == replicate).all(axis=1)]\n",
    "    df_train = df[(df[group_columns] != replicate).any(axis=1)]\n",
    "    \n",
    "#     print(len(df_train), len(df_val), len(df), df_train.cell_class.unique())\n",
    "\n",
    "    # cell_class is target\n",
    "    conditions = df_train.cell_class\n",
    "    conditions_val = df_val.cell_class\n",
    "    \n",
    "    # to numeric labels\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(conditions)\n",
    "    y_val = le.transform(conditions_val)\n",
    "\n",
    " # + [c for c in df.columns if not 'LBP' in c ]\n",
    "\n",
    "    x_train = df_train.drop(columns_to_drop, 1).values\n",
    "    x_val = df_val.drop(columns_to_drop, 1).values\n",
    "\n",
    "    # impute NaNs and normalize\n",
    "    x_train = SimpleImputer().fit_transform(x_train)\n",
    "    x_val = SimpleImputer().fit_transform(x_val)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_val = sc.transform(x_val)\n",
    "\n",
    "    # train new cls and val\n",
    "    cls = clone(cls)\n",
    "    cls.fit(x_train, y_train)\n",
    "    score = cls.score(x_val, y_val)\n",
    "    \n",
    "    res[replicate] = score\n",
    "    \n",
    "    print(f'({i+1}/{len(replicates)}): {replicate}')\n",
    "    \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140aea2e-e87d-4fc4-889e-bec062d21fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list(res.values())\n",
    "repl_size = [(df[group_columns] == replicate).all(axis=1).sum() for replicate in replicates]\n",
    "\n",
    "(np.array(scores) * np.array(repl_size)).sum() / np.sum(repl_size) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
